{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "import numpy as np \n",
    "import deepcell\n",
    "from deepcell_toolbox.multiplex_utils import multiplex_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder for this set of experiments\n",
    "experiment_folder = \"size_benchmarking\"\n",
    "MODEL_DIR = os.path.join(\"/data/analyses\", experiment_folder)\n",
    "NPZ_DIR = \"/data/npz_data/20201018_freeze/\"\n",
    "LOG_DIR = '/data/logs'\n",
    "\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.utils.data_utils import get_data\n",
    "from skimage.segmentation import relabel_sequential\n",
    "\n",
    "npz_name = \"20201018_multiplex_seed_2_\"\n",
    "\n",
    "train_dict = np.load(NPZ_DIR + npz_name + \"train_512x512_split.npz\", allow_pickle=True)\n",
    "val_dict = np.load(NPZ_DIR + npz_name + \"val_256x256_split.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '3', '10', '33', '100', '333', '1000', '2665']\n"
     ]
    }
   ],
   "source": [
    "train_keys, val_keys = list(train_dict.keys()), list(val_dict.keys())\n",
    "print(train_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for 1000\n",
      "X_train shape is (1000, 512, 512, 2), y_train shape is (1000, 512, 512, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1027 04:30:31.265104 139858664515392 semantic.py:111] X data dtype is float32: this will increase memory use during preprocessing. Consider using a smaller dtype\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val shape is (1230, 256, 256, 2), y_val shape is (1230, 256, 256, 1)\n",
      "Model name is 20201018_multiplex_seed_2__subset_1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1027 04:33:14.317371 139858664515392 semantic.py:111] X data dtype is float32: this will increase memory use during preprocessing. Consider using a smaller dtype\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generators created\n",
      "Training on 1 GPUs.\n",
      "Epoch 1/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0810 - semantic_0_loss: 0.0735 - semantic_1_loss: 0.0075\n",
      "Epoch 00001: val_loss improved from inf to 0.15149, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 958s 3s/step - loss: 0.0808 - semantic_0_loss: 0.0733 - semantic_1_loss: 0.0075 - val_loss: 0.1515 - val_semantic_0_loss: 0.0513 - val_semantic_1_loss: 0.1001\n",
      "Epoch 2/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0251 - semantic_0_loss: 0.0188 - semantic_1_loss: 0.0062\n",
      "Epoch 00002: val_loss improved from 0.15149 to 0.05394, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 89s 268ms/step - loss: 0.0251 - semantic_0_loss: 0.0188 - semantic_1_loss: 0.0062 - val_loss: 0.0539 - val_semantic_0_loss: 0.0366 - val_semantic_1_loss: 0.0174\n",
      "Epoch 3/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0219 - semantic_0_loss: 0.0160 - semantic_1_loss: 0.0059\n",
      "Epoch 00003: val_loss improved from 0.05394 to 0.01832, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 89s 266ms/step - loss: 0.0219 - semantic_0_loss: 0.0160 - semantic_1_loss: 0.0059 - val_loss: 0.0183 - val_semantic_0_loss: 0.0131 - val_semantic_1_loss: 0.0052\n",
      "Epoch 4/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0211 - semantic_0_loss: 0.0155 - semantic_1_loss: 0.0056\n",
      "Epoch 00004: val_loss improved from 0.01832 to 0.01660, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 90s 271ms/step - loss: 0.0211 - semantic_0_loss: 0.0155 - semantic_1_loss: 0.0056 - val_loss: 0.0166 - val_semantic_0_loss: 0.0119 - val_semantic_1_loss: 0.0047\n",
      "Epoch 5/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0203 - semantic_0_loss: 0.0149 - semantic_1_loss: 0.0054\n",
      "Epoch 00005: val_loss improved from 0.01660 to 0.01574, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 88s 264ms/step - loss: 0.0203 - semantic_0_loss: 0.0149 - semantic_1_loss: 0.0054 - val_loss: 0.0157 - val_semantic_0_loss: 0.0112 - val_semantic_1_loss: 0.0045\n",
      "Epoch 6/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0196 - semantic_0_loss: 0.0143 - semantic_1_loss: 0.0054\n",
      "Epoch 00006: val_loss did not improve from 0.01574\n",
      "333/333 [==============================] - 83s 250ms/step - loss: 0.0196 - semantic_0_loss: 0.0143 - semantic_1_loss: 0.0054 - val_loss: 0.0172 - val_semantic_0_loss: 0.0121 - val_semantic_1_loss: 0.0051\n",
      "Epoch 7/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0187 - semantic_0_loss: 0.0135 - semantic_1_loss: 0.0052\n",
      "Epoch 00007: val_loss improved from 0.01574 to 0.01521, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 89s 268ms/step - loss: 0.0187 - semantic_0_loss: 0.0135 - semantic_1_loss: 0.0052 - val_loss: 0.0152 - val_semantic_0_loss: 0.0107 - val_semantic_1_loss: 0.0045\n",
      "Epoch 8/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0184 - semantic_0_loss: 0.0132 - semantic_1_loss: 0.0052\n",
      "Epoch 00008: val_loss did not improve from 0.01521\n",
      "333/333 [==============================] - 83s 248ms/step - loss: 0.0184 - semantic_0_loss: 0.0132 - semantic_1_loss: 0.0052 - val_loss: 0.0157 - val_semantic_0_loss: 0.0112 - val_semantic_1_loss: 0.0046\n",
      "Epoch 9/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0180 - semantic_0_loss: 0.0129 - semantic_1_loss: 0.0051\n",
      "Epoch 00009: val_loss improved from 0.01521 to 0.01520, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 88s 263ms/step - loss: 0.0180 - semantic_0_loss: 0.0129 - semantic_1_loss: 0.0051 - val_loss: 0.0152 - val_semantic_0_loss: 0.0108 - val_semantic_1_loss: 0.0044\n",
      "Epoch 10/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0175 - semantic_0_loss: 0.0125 - semantic_1_loss: 0.0050\n",
      "Epoch 00010: val_loss improved from 0.01520 to 0.01490, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 89s 268ms/step - loss: 0.0175 - semantic_0_loss: 0.0125 - semantic_1_loss: 0.0050 - val_loss: 0.0149 - val_semantic_0_loss: 0.0105 - val_semantic_1_loss: 0.0044\n",
      "Epoch 11/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0174 - semantic_0_loss: 0.0124 - semantic_1_loss: 0.0050\n",
      "Epoch 00011: val_loss improved from 0.01490 to 0.01421, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 87s 262ms/step - loss: 0.0174 - semantic_0_loss: 0.0124 - semantic_1_loss: 0.0050 - val_loss: 0.0142 - val_semantic_0_loss: 0.0100 - val_semantic_1_loss: 0.0043\n",
      "Epoch 12/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0169 - semantic_0_loss: 0.0120 - semantic_1_loss: 0.0049\n",
      "Epoch 00012: val_loss did not improve from 0.01421\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0169 - semantic_0_loss: 0.0120 - semantic_1_loss: 0.0049 - val_loss: 0.0163 - val_semantic_0_loss: 0.0113 - val_semantic_1_loss: 0.0050\n",
      "Epoch 13/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0169 - semantic_0_loss: 0.0120 - semantic_1_loss: 0.0049\n",
      "Epoch 00013: val_loss did not improve from 0.01421\n",
      "333/333 [==============================] - 84s 251ms/step - loss: 0.0169 - semantic_0_loss: 0.0120 - semantic_1_loss: 0.0049 - val_loss: 0.0575 - val_semantic_0_loss: 0.0526 - val_semantic_1_loss: 0.0049\n",
      "Epoch 14/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0169 - semantic_0_loss: 0.0119 - semantic_1_loss: 0.0049\n",
      "Epoch 00014: val_loss improved from 0.01421 to 0.01368, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 87s 261ms/step - loss: 0.0169 - semantic_0_loss: 0.0119 - semantic_1_loss: 0.0049 - val_loss: 0.0137 - val_semantic_0_loss: 0.0095 - val_semantic_1_loss: 0.0042\n",
      "Epoch 15/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0167 - semantic_0_loss: 0.0119 - semantic_1_loss: 0.0049\n",
      "Epoch 00015: val_loss did not improve from 0.01368\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0167 - semantic_0_loss: 0.0118 - semantic_1_loss: 0.0049 - val_loss: 0.0156 - val_semantic_0_loss: 0.0110 - val_semantic_1_loss: 0.0046\n",
      "Epoch 16/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0164 - semantic_0_loss: 0.0116 - semantic_1_loss: 0.0048\n",
      "Epoch 00016: val_loss did not improve from 0.01368\n",
      "333/333 [==============================] - 84s 251ms/step - loss: 0.0164 - semantic_0_loss: 0.0116 - semantic_1_loss: 0.0048 - val_loss: 0.0139 - val_semantic_0_loss: 0.0097 - val_semantic_1_loss: 0.0042\n",
      "Epoch 17/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0165 - semantic_0_loss: 0.0117 - semantic_1_loss: 0.0049\n",
      "Epoch 00017: val_loss did not improve from 0.01368\n",
      "333/333 [==============================] - 82s 247ms/step - loss: 0.0165 - semantic_0_loss: 0.0117 - semantic_1_loss: 0.0049 - val_loss: 0.0140 - val_semantic_0_loss: 0.0098 - val_semantic_1_loss: 0.0042\n",
      "Epoch 18/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0163 - semantic_0_loss: 0.0115 - semantic_1_loss: 0.0048\n",
      "Epoch 00018: val_loss did not improve from 0.01368\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0163 - semantic_0_loss: 0.0115 - semantic_1_loss: 0.0048 - val_loss: 0.0137 - val_semantic_0_loss: 0.0096 - val_semantic_1_loss: 0.0042\n",
      "Epoch 19/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0162 - semantic_0_loss: 0.0114 - semantic_1_loss: 0.0048\n",
      "Epoch 00019: val_loss improved from 0.01368 to 0.01360, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333/333 [==============================] - 88s 265ms/step - loss: 0.0162 - semantic_0_loss: 0.0114 - semantic_1_loss: 0.0048 - val_loss: 0.0136 - val_semantic_0_loss: 0.0094 - val_semantic_1_loss: 0.0042\n",
      "Epoch 20/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0159 - semantic_0_loss: 0.0112 - semantic_1_loss: 0.0047\n",
      "Epoch 00020: val_loss improved from 0.01360 to 0.01356, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 87s 263ms/step - loss: 0.0159 - semantic_0_loss: 0.0112 - semantic_1_loss: 0.0047 - val_loss: 0.0136 - val_semantic_0_loss: 0.0094 - val_semantic_1_loss: 0.0041\n",
      "Epoch 21/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0156 - semantic_0_loss: 0.0110 - semantic_1_loss: 0.0047\n",
      "Epoch 00021: val_loss did not improve from 0.01356\n",
      "333/333 [==============================] - 82s 245ms/step - loss: 0.0156 - semantic_0_loss: 0.0110 - semantic_1_loss: 0.0047 - val_loss: 0.0136 - val_semantic_0_loss: 0.0094 - val_semantic_1_loss: 0.0041\n",
      "Epoch 22/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0158 - semantic_0_loss: 0.0111 - semantic_1_loss: 0.0047\n",
      "Epoch 00022: val_loss improved from 0.01356 to 0.01336, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 88s 265ms/step - loss: 0.0158 - semantic_0_loss: 0.0111 - semantic_1_loss: 0.0047 - val_loss: 0.0134 - val_semantic_0_loss: 0.0093 - val_semantic_1_loss: 0.0041\n",
      "Epoch 23/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0155 - semantic_0_loss: 0.0109 - semantic_1_loss: 0.0047\n",
      "Epoch 00023: val_loss did not improve from 0.01336\n",
      "333/333 [==============================] - 82s 245ms/step - loss: 0.0155 - semantic_0_loss: 0.0109 - semantic_1_loss: 0.0047 - val_loss: 0.0135 - val_semantic_0_loss: 0.0094 - val_semantic_1_loss: 0.0041\n",
      "Epoch 24/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0156 - semantic_0_loss: 0.0110 - semantic_1_loss: 0.0047\n",
      "Epoch 00024: val_loss improved from 0.01336 to 0.01328, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 87s 260ms/step - loss: 0.0156 - semantic_0_loss: 0.0110 - semantic_1_loss: 0.0047 - val_loss: 0.0133 - val_semantic_0_loss: 0.0092 - val_semantic_1_loss: 0.0040\n",
      "Epoch 25/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0156 - semantic_0_loss: 0.0109 - semantic_1_loss: 0.0047\n",
      "Epoch 00025: val_loss did not improve from 0.01328\n",
      "333/333 [==============================] - 83s 249ms/step - loss: 0.0156 - semantic_0_loss: 0.0109 - semantic_1_loss: 0.0046 - val_loss: 0.0151 - val_semantic_0_loss: 0.0109 - val_semantic_1_loss: 0.0042\n",
      "Epoch 26/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0155 - semantic_0_loss: 0.0108 - semantic_1_loss: 0.0046\n",
      "Epoch 00026: val_loss did not improve from 0.01328\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0155 - semantic_0_loss: 0.0108 - semantic_1_loss: 0.0046 - val_loss: 0.0254 - val_semantic_0_loss: 0.0188 - val_semantic_1_loss: 0.0065\n",
      "Epoch 27/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0154 - semantic_0_loss: 0.0107 - semantic_1_loss: 0.0046\n",
      "Epoch 00027: val_loss did not improve from 0.01328\n",
      "333/333 [==============================] - 82s 245ms/step - loss: 0.0154 - semantic_0_loss: 0.0107 - semantic_1_loss: 0.0046 - val_loss: 0.0334 - val_semantic_0_loss: 0.0283 - val_semantic_1_loss: 0.0051\n",
      "Epoch 28/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0154 - semantic_0_loss: 0.0107 - semantic_1_loss: 0.0046\n",
      "Epoch 00028: val_loss improved from 0.01328 to 0.01292, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 88s 263ms/step - loss: 0.0154 - semantic_0_loss: 0.0107 - semantic_1_loss: 0.0046 - val_loss: 0.0129 - val_semantic_0_loss: 0.0089 - val_semantic_1_loss: 0.0040\n",
      "Epoch 29/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0152 - semantic_0_loss: 0.0106 - semantic_1_loss: 0.0046\n",
      "Epoch 00029: val_loss did not improve from 0.01292\n",
      "333/333 [==============================] - 81s 244ms/step - loss: 0.0152 - semantic_0_loss: 0.0106 - semantic_1_loss: 0.0046 - val_loss: 0.0130 - val_semantic_0_loss: 0.0090 - val_semantic_1_loss: 0.0040\n",
      "Epoch 30/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0150 - semantic_0_loss: 0.0105 - semantic_1_loss: 0.0046\n",
      "Epoch 00030: val_loss improved from 0.01292 to 0.01292, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 86s 258ms/step - loss: 0.0150 - semantic_0_loss: 0.0105 - semantic_1_loss: 0.0046 - val_loss: 0.0129 - val_semantic_0_loss: 0.0089 - val_semantic_1_loss: 0.0040\n",
      "Epoch 31/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0151 - semantic_0_loss: 0.0105 - semantic_1_loss: 0.0046\n",
      "Epoch 00031: val_loss did not improve from 0.01292\n",
      "333/333 [==============================] - 82s 248ms/step - loss: 0.0151 - semantic_0_loss: 0.0105 - semantic_1_loss: 0.0046 - val_loss: 0.0266 - val_semantic_0_loss: 0.0189 - val_semantic_1_loss: 0.0077\n",
      "Epoch 32/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0151 - semantic_0_loss: 0.0105 - semantic_1_loss: 0.0046\n",
      "Epoch 00032: val_loss did not improve from 0.01292\n",
      "333/333 [==============================] - 81s 243ms/step - loss: 0.0150 - semantic_0_loss: 0.0105 - semantic_1_loss: 0.0046 - val_loss: 0.0161 - val_semantic_0_loss: 0.0111 - val_semantic_1_loss: 0.0050\n",
      "Epoch 33/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0149 - semantic_0_loss: 0.0104 - semantic_1_loss: 0.0045\n",
      "Epoch 00033: val_loss did not improve from 0.01292\n",
      "333/333 [==============================] - 81s 245ms/step - loss: 0.0149 - semantic_0_loss: 0.0104 - semantic_1_loss: 0.0045 - val_loss: 0.0133 - val_semantic_0_loss: 0.0092 - val_semantic_1_loss: 0.0041\n",
      "Epoch 34/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0149 - semantic_0_loss: 0.0103 - semantic_1_loss: 0.0045\n",
      "Epoch 00034: val_loss improved from 0.01292 to 0.01278, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 87s 262ms/step - loss: 0.0149 - semantic_0_loss: 0.0103 - semantic_1_loss: 0.0045 - val_loss: 0.0128 - val_semantic_0_loss: 0.0088 - val_semantic_1_loss: 0.0039\n",
      "Epoch 35/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0149 - semantic_0_loss: 0.0104 - semantic_1_loss: 0.0045\n",
      "Epoch 00035: val_loss did not improve from 0.01278\n",
      "333/333 [==============================] - 81s 243ms/step - loss: 0.0149 - semantic_0_loss: 0.0104 - semantic_1_loss: 0.0045 - val_loss: 0.0259 - val_semantic_0_loss: 0.0217 - val_semantic_1_loss: 0.0041\n",
      "Epoch 36/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0148 - semantic_0_loss: 0.0103 - semantic_1_loss: 0.0045\n",
      "Epoch 00036: val_loss did not improve from 0.01278\n",
      "333/333 [==============================] - 80s 241ms/step - loss: 0.0148 - semantic_0_loss: 0.0103 - semantic_1_loss: 0.0045 - val_loss: 0.0154 - val_semantic_0_loss: 0.0100 - val_semantic_1_loss: 0.0054\n",
      "Epoch 37/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0148 - semantic_0_loss: 0.0103 - semantic_1_loss: 0.0045\n",
      "Epoch 00037: val_loss did not improve from 0.01278\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0148 - semantic_0_loss: 0.0103 - semantic_1_loss: 0.0045 - val_loss: 0.0280 - val_semantic_0_loss: 0.0236 - val_semantic_1_loss: 0.0044\n",
      "Epoch 38/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0148 - semantic_0_loss: 0.0103 - semantic_1_loss: 0.0045\n",
      "Epoch 00038: val_loss improved from 0.01278 to 0.01271, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 86s 257ms/step - loss: 0.0148 - semantic_0_loss: 0.0103 - semantic_1_loss: 0.0045 - val_loss: 0.0127 - val_semantic_0_loss: 0.0088 - val_semantic_1_loss: 0.0039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0148 - semantic_0_loss: 0.0103 - semantic_1_loss: 0.0045\n",
      "Epoch 00039: val_loss did not improve from 0.01271\n",
      "333/333 [==============================] - 82s 245ms/step - loss: 0.0148 - semantic_0_loss: 0.0103 - semantic_1_loss: 0.0045 - val_loss: 0.0128 - val_semantic_0_loss: 0.0089 - val_semantic_1_loss: 0.0040\n",
      "Epoch 40/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0148 - semantic_0_loss: 0.0103 - semantic_1_loss: 0.0045\n",
      "Epoch 00040: val_loss improved from 0.01271 to 0.01262, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 87s 261ms/step - loss: 0.0148 - semantic_0_loss: 0.0103 - semantic_1_loss: 0.0045 - val_loss: 0.0126 - val_semantic_0_loss: 0.0087 - val_semantic_1_loss: 0.0039\n",
      "Epoch 41/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0145 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0044\n",
      "Epoch 00041: val_loss did not improve from 0.01262\n",
      "333/333 [==============================] - 81s 244ms/step - loss: 0.0145 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0044 - val_loss: 0.0128 - val_semantic_0_loss: 0.0088 - val_semantic_1_loss: 0.0040\n",
      "Epoch 42/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0147 - semantic_0_loss: 0.0103 - semantic_1_loss: 0.0045\n",
      "Epoch 00042: val_loss did not improve from 0.01262\n",
      "333/333 [==============================] - 82s 245ms/step - loss: 0.0147 - semantic_0_loss: 0.0103 - semantic_1_loss: 0.0045 - val_loss: 0.0127 - val_semantic_0_loss: 0.0087 - val_semantic_1_loss: 0.0040\n",
      "Epoch 43/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0146 - semantic_0_loss: 0.0102 - semantic_1_loss: 0.0045\n",
      "Epoch 00043: val_loss did not improve from 0.01262\n",
      "333/333 [==============================] - 83s 248ms/step - loss: 0.0146 - semantic_0_loss: 0.0102 - semantic_1_loss: 0.0045 - val_loss: 0.0134 - val_semantic_0_loss: 0.0092 - val_semantic_1_loss: 0.0041\n",
      "Epoch 44/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0147 - semantic_0_loss: 0.0102 - semantic_1_loss: 0.0045\n",
      "Epoch 00044: val_loss did not improve from 0.01262\n",
      "333/333 [==============================] - 82s 245ms/step - loss: 0.0147 - semantic_0_loss: 0.0102 - semantic_1_loss: 0.0045 - val_loss: 0.0129 - val_semantic_0_loss: 0.0089 - val_semantic_1_loss: 0.0040\n",
      "Epoch 45/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0146 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0044\n",
      "Epoch 00045: val_loss improved from 0.01262 to 0.01248, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 86s 259ms/step - loss: 0.0146 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0044 - val_loss: 0.0125 - val_semantic_0_loss: 0.0086 - val_semantic_1_loss: 0.0039\n",
      "Epoch 46/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0146 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0044\n",
      "Epoch 00046: val_loss did not improve from 0.01248\n",
      "333/333 [==============================] - 82s 247ms/step - loss: 0.0146 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0045 - val_loss: 0.0127 - val_semantic_0_loss: 0.0088 - val_semantic_1_loss: 0.0040\n",
      "Epoch 47/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0145 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0044\n",
      "Epoch 00047: val_loss did not improve from 0.01248\n",
      "333/333 [==============================] - 81s 243ms/step - loss: 0.0145 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0044 - val_loss: 0.0128 - val_semantic_0_loss: 0.0088 - val_semantic_1_loss: 0.0040\n",
      "Epoch 48/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0145 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0044\n",
      "Epoch 00048: val_loss did not improve from 0.01248\n",
      "333/333 [==============================] - 82s 245ms/step - loss: 0.0145 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0044 - val_loss: 0.0126 - val_semantic_0_loss: 0.0087 - val_semantic_1_loss: 0.0039\n",
      "Epoch 49/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0145 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0045\n",
      "Epoch 00049: val_loss improved from 0.01248 to 0.01246, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 87s 262ms/step - loss: 0.0145 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0045 - val_loss: 0.0125 - val_semantic_0_loss: 0.0086 - val_semantic_1_loss: 0.0039\n",
      "Epoch 50/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0143 - semantic_0_loss: 0.0100 - semantic_1_loss: 0.0044\n",
      "Epoch 00050: val_loss did not improve from 0.01246\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0144 - semantic_0_loss: 0.0100 - semantic_1_loss: 0.0044 - val_loss: 0.0126 - val_semantic_0_loss: 0.0086 - val_semantic_1_loss: 0.0039\n",
      "Epoch 51/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0144 - semantic_0_loss: 0.0100 - semantic_1_loss: 0.0044\n",
      "Epoch 00051: val_loss did not improve from 0.01246\n",
      "333/333 [==============================] - 81s 243ms/step - loss: 0.0144 - semantic_0_loss: 0.0100 - semantic_1_loss: 0.0044 - val_loss: 0.0141 - val_semantic_0_loss: 0.0102 - val_semantic_1_loss: 0.0039\n",
      "Epoch 52/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0144 - semantic_0_loss: 0.0100 - semantic_1_loss: 0.0044\n",
      "Epoch 00052: val_loss improved from 0.01246 to 0.01242, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 87s 260ms/step - loss: 0.0144 - semantic_0_loss: 0.0100 - semantic_1_loss: 0.0044 - val_loss: 0.0124 - val_semantic_0_loss: 0.0085 - val_semantic_1_loss: 0.0039\n",
      "Epoch 53/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0143 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044\n",
      "Epoch 00053: val_loss did not improve from 0.01242\n",
      "333/333 [==============================] - 81s 244ms/step - loss: 0.0143 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044 - val_loss: 0.0125 - val_semantic_0_loss: 0.0086 - val_semantic_1_loss: 0.0039\n",
      "Epoch 54/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0142 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044\n",
      "Epoch 00054: val_loss did not improve from 0.01242\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0143 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044 - val_loss: 0.0127 - val_semantic_0_loss: 0.0087 - val_semantic_1_loss: 0.0040\n",
      "Epoch 55/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0141 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0044\n",
      "Epoch 00055: val_loss did not improve from 0.01242\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0142 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0044 - val_loss: 0.0125 - val_semantic_0_loss: 0.0087 - val_semantic_1_loss: 0.0038\n",
      "Epoch 56/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0145 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0044\n",
      "Epoch 00056: val_loss did not improve from 0.01242\n",
      "333/333 [==============================] - 81s 243ms/step - loss: 0.0145 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0044 - val_loss: 0.0147 - val_semantic_0_loss: 0.0108 - val_semantic_1_loss: 0.0040\n",
      "Epoch 57/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0142 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0044\n",
      "Epoch 00057: val_loss did not improve from 0.01242\n",
      "333/333 [==============================] - 81s 242ms/step - loss: 0.0142 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0044 - val_loss: 0.0126 - val_semantic_0_loss: 0.0087 - val_semantic_1_loss: 0.0039\n",
      "Epoch 58/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0142 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0043\n",
      "Epoch 00058: val_loss did not improve from 0.01242\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0142 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0043 - val_loss: 0.0124 - val_semantic_0_loss: 0.0086 - val_semantic_1_loss: 0.0039\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332/333 [============================>.] - ETA: 0s - loss: 0.0143 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044\n",
      "Epoch 00059: val_loss did not improve from 0.01242\n",
      "333/333 [==============================] - 81s 244ms/step - loss: 0.0143 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044 - val_loss: 0.0126 - val_semantic_0_loss: 0.0087 - val_semantic_1_loss: 0.0039\n",
      "Epoch 60/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0140 - semantic_0_loss: 0.0097 - semantic_1_loss: 0.0044\n",
      "Epoch 00060: val_loss did not improve from 0.01242\n",
      "333/333 [==============================] - 81s 242ms/step - loss: 0.0140 - semantic_0_loss: 0.0097 - semantic_1_loss: 0.0044 - val_loss: 0.0129 - val_semantic_0_loss: 0.0090 - val_semantic_1_loss: 0.0039\n",
      "Epoch 61/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0143 - semantic_0_loss: 0.0100 - semantic_1_loss: 0.0044\n",
      "Epoch 00061: val_loss did not improve from 0.01242\n",
      "333/333 [==============================] - 81s 244ms/step - loss: 0.0143 - semantic_0_loss: 0.0100 - semantic_1_loss: 0.0044 - val_loss: 0.0126 - val_semantic_0_loss: 0.0086 - val_semantic_1_loss: 0.0040\n",
      "Epoch 62/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0140 - semantic_0_loss: 0.0097 - semantic_1_loss: 0.0043\n",
      "Epoch 00062: val_loss improved from 0.01242 to 0.01235, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 85s 257ms/step - loss: 0.0140 - semantic_0_loss: 0.0097 - semantic_1_loss: 0.0043 - val_loss: 0.0124 - val_semantic_0_loss: 0.0085 - val_semantic_1_loss: 0.0039\n",
      "Epoch 63/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0141 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0044\n",
      "Epoch 00063: val_loss did not improve from 0.01235\n",
      "333/333 [==============================] - 81s 243ms/step - loss: 0.0141 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0044 - val_loss: 0.0124 - val_semantic_0_loss: 0.0085 - val_semantic_1_loss: 0.0039\n",
      "Epoch 64/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0140 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043\n",
      "Epoch 00064: val_loss did not improve from 0.01235\n",
      "333/333 [==============================] - 81s 244ms/step - loss: 0.0140 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043 - val_loss: 0.0125 - val_semantic_0_loss: 0.0086 - val_semantic_1_loss: 0.0039\n",
      "Epoch 65/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0142 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0044\n",
      "Epoch 00065: val_loss did not improve from 0.01235\n",
      "333/333 [==============================] - 81s 244ms/step - loss: 0.0141 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0044 - val_loss: 0.0155 - val_semantic_0_loss: 0.0116 - val_semantic_1_loss: 0.0039\n",
      "Epoch 66/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0140 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043\n",
      "Epoch 00066: val_loss did not improve from 0.01235\n",
      "333/333 [==============================] - 81s 244ms/step - loss: 0.0140 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043 - val_loss: 0.0124 - val_semantic_0_loss: 0.0085 - val_semantic_1_loss: 0.0039\n",
      "Epoch 67/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0142 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044\n",
      "Epoch 00067: val_loss did not improve from 0.01235\n",
      "333/333 [==============================] - 81s 244ms/step - loss: 0.0142 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044 - val_loss: 0.0151 - val_semantic_0_loss: 0.0100 - val_semantic_1_loss: 0.0051\n",
      "Epoch 68/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043\n",
      "Epoch 00068: val_loss did not improve from 0.01235\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043 - val_loss: 0.0124 - val_semantic_0_loss: 0.0085 - val_semantic_1_loss: 0.0038\n",
      "Epoch 69/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0140 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043\n",
      "Epoch 00069: val_loss did not improve from 0.01235\n",
      "333/333 [==============================] - 83s 250ms/step - loss: 0.0140 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043 - val_loss: 0.0124 - val_semantic_0_loss: 0.0085 - val_semantic_1_loss: 0.0038\n",
      "Epoch 70/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0141 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0043\n",
      "Epoch 00070: val_loss improved from 0.01235 to 0.01232, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 88s 264ms/step - loss: 0.0141 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0043 - val_loss: 0.0123 - val_semantic_0_loss: 0.0085 - val_semantic_1_loss: 0.0039\n",
      "Epoch 71/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0138 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043\n",
      "Epoch 00071: val_loss improved from 0.01232 to 0.01223, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 87s 262ms/step - loss: 0.0138 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043 - val_loss: 0.0122 - val_semantic_0_loss: 0.0084 - val_semantic_1_loss: 0.0038\n",
      "Epoch 72/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043\n",
      "Epoch 00072: val_loss did not improve from 0.01223\n",
      "333/333 [==============================] - 81s 244ms/step - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043 - val_loss: 0.0123 - val_semantic_0_loss: 0.0085 - val_semantic_1_loss: 0.0038\n",
      "Epoch 73/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0140 - semantic_0_loss: 0.0097 - semantic_1_loss: 0.0043\n",
      "Epoch 00073: val_loss did not improve from 0.01223\n",
      "333/333 [==============================] - 82s 245ms/step - loss: 0.0140 - semantic_0_loss: 0.0097 - semantic_1_loss: 0.0043 - val_loss: 0.0122 - val_semantic_0_loss: 0.0084 - val_semantic_1_loss: 0.0038\n",
      "Epoch 74/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0138 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043\n",
      "Epoch 00074: val_loss improved from 0.01223 to 0.01215, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_1000.h5\n",
      "333/333 [==============================] - 86s 260ms/step - loss: 0.0138 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043 - val_loss: 0.0122 - val_semantic_0_loss: 0.0084 - val_semantic_1_loss: 0.0038\n",
      "Epoch 75/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0138 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043\n",
      "Epoch 00075: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 81s 243ms/step - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043 - val_loss: 0.0123 - val_semantic_0_loss: 0.0085 - val_semantic_1_loss: 0.0038\n",
      "Epoch 76/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0140 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043\n",
      "Epoch 00076: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 81s 245ms/step - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043 - val_loss: 0.0123 - val_semantic_0_loss: 0.0084 - val_semantic_1_loss: 0.0038\n",
      "Epoch 77/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043\n",
      "Epoch 00077: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043 - val_loss: 0.0125 - val_semantic_0_loss: 0.0086 - val_semantic_1_loss: 0.0040\n",
      "Epoch 78/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043\n",
      "Epoch 00078: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043 - val_loss: 0.0125 - val_semantic_0_loss: 0.0087 - val_semantic_1_loss: 0.0038\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332/333 [============================>.] - ETA: 0s - loss: 0.0137 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043\n",
      "Epoch 00079: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0137 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043 - val_loss: 0.0157 - val_semantic_0_loss: 0.0109 - val_semantic_1_loss: 0.0047\n",
      "Epoch 80/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0138 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0042\n",
      "Epoch 00080: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 82s 247ms/step - loss: 0.0138 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0042 - val_loss: 0.0124 - val_semantic_0_loss: 0.0086 - val_semantic_1_loss: 0.0038\n",
      "Epoch 81/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0138 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043\n",
      "Epoch 00081: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0137 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043 - val_loss: 0.0122 - val_semantic_0_loss: 0.0084 - val_semantic_1_loss: 0.0038\n",
      "Epoch 82/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043\n",
      "Epoch 00082: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043 - val_loss: 0.0127 - val_semantic_0_loss: 0.0088 - val_semantic_1_loss: 0.0039\n",
      "Epoch 83/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0137 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0042\n",
      "Epoch 00083: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 82s 245ms/step - loss: 0.0137 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0042 - val_loss: 0.0122 - val_semantic_0_loss: 0.0083 - val_semantic_1_loss: 0.0038\n",
      "Epoch 84/100\n",
      " 73/333 [=====>........................] - ETA: 50s - loss: 0.0141 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0043\n",
      "Epoch 00084: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 82s 246ms/step - loss: 0.0138 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043 - val_loss: 0.0123 - val_semantic_0_loss: 0.0084 - val_semantic_1_loss: 0.0038\n",
      "Epoch 85/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0138 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043\n",
      "Epoch 00085: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 81s 242ms/step - loss: 0.0137 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043 - val_loss: 0.0123 - val_semantic_0_loss: 0.0084 - val_semantic_1_loss: 0.0039\n",
      "Epoch 86/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0136 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0042\n",
      "Epoch 00086: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 81s 242ms/step - loss: 0.0136 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0042 - val_loss: 0.0123 - val_semantic_0_loss: 0.0084 - val_semantic_1_loss: 0.0038\n",
      "Epoch 87/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0137 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0043\n",
      "Epoch 00087: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 81s 243ms/step - loss: 0.0137 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0043 - val_loss: 0.0122 - val_semantic_0_loss: 0.0084 - val_semantic_1_loss: 0.0039\n",
      "Epoch 88/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0138 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0042\n",
      "Epoch 00088: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 82s 245ms/step - loss: 0.0137 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0042 - val_loss: 0.0122 - val_semantic_0_loss: 0.0084 - val_semantic_1_loss: 0.0038\n",
      "Epoch 89/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0136 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0042\n",
      "Epoch 00089: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 81s 244ms/step - loss: 0.0136 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0042 - val_loss: 0.0136 - val_semantic_0_loss: 0.0092 - val_semantic_1_loss: 0.0044\n",
      "Epoch 90/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0137 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0042\n",
      "Epoch 00090: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 82s 245ms/step - loss: 0.0137 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0042 - val_loss: 0.0124 - val_semantic_0_loss: 0.0085 - val_semantic_1_loss: 0.0039\n",
      "Epoch 91/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0135 - semantic_0_loss: 0.0093 - semantic_1_loss: 0.0042\n",
      "Epoch 00091: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 81s 243ms/step - loss: 0.0135 - semantic_0_loss: 0.0093 - semantic_1_loss: 0.0042 - val_loss: 0.0125 - val_semantic_0_loss: 0.0086 - val_semantic_1_loss: 0.0038\n",
      "Epoch 92/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0138 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0042\n",
      "Epoch 00092: val_loss did not improve from 0.01215\n",
      "333/333 [==============================] - 82s 247ms/step - loss: 0.0138 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0042 - val_loss: 0.0125 - val_semantic_0_loss: 0.0085 - val_semantic_1_loss: 0.0040\n",
      "Epoch 93/100\n",
      "326/333 [============================>.] - ETA: 1s - loss: 0.0136 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332/333 [============================>.] - ETA: 0s - loss: 0.0203 - semantic_0_loss: 0.0147 - semantic_1_loss: 0.0055\n",
      "Epoch 00003: val_loss improved from 0.06243 to 0.01820, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_2665.h5\n",
      "333/333 [==============================] - 107s 322ms/step - loss: 0.0203 - semantic_0_loss: 0.0147 - semantic_1_loss: 0.0055 - val_loss: 0.0182 - val_semantic_0_loss: 0.0126 - val_semantic_1_loss: 0.0056\n",
      "Epoch 4/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0193 - semantic_0_loss: 0.0139 - semantic_1_loss: 0.0054\n",
      "Epoch 00004: val_loss improved from 0.01820 to 0.01557, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_2665.h5\n",
      "333/333 [==============================] - 107s 323ms/step - loss: 0.0193 - semantic_0_loss: 0.0139 - semantic_1_loss: 0.0054 - val_loss: 0.0156 - val_semantic_0_loss: 0.0110 - val_semantic_1_loss: 0.0046\n",
      "Epoch 5/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0188 - semantic_0_loss: 0.0135 - semantic_1_loss: 0.0053\n",
      "Epoch 00005: val_loss did not improve from 0.01557\n",
      "333/333 [==============================] - 102s 306ms/step - loss: 0.0188 - semantic_0_loss: 0.0135 - semantic_1_loss: 0.0053 - val_loss: 0.0210 - val_semantic_0_loss: 0.0129 - val_semantic_1_loss: 0.0081\n",
      "Epoch 6/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0183 - semantic_0_loss: 0.0131 - semantic_1_loss: 0.0051\n",
      "Epoch 00006: val_loss improved from 0.01557 to 0.01459, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_2665.h5\n",
      "333/333 [==============================] - 106s 320ms/step - loss: 0.0183 - semantic_0_loss: 0.0131 - semantic_1_loss: 0.0051 - val_loss: 0.0146 - val_semantic_0_loss: 0.0102 - val_semantic_1_loss: 0.0044\n",
      "Epoch 7/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0177 - semantic_0_loss: 0.0126 - semantic_1_loss: 0.0051\n",
      "Epoch 00007: val_loss did not improve from 0.01459\n",
      "333/333 [==============================] - 102s 306ms/step - loss: 0.0177 - semantic_0_loss: 0.0126 - semantic_1_loss: 0.0051 - val_loss: 0.0393 - val_semantic_0_loss: 0.0342 - val_semantic_1_loss: 0.0051\n",
      "Epoch 8/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0176 - semantic_0_loss: 0.0125 - semantic_1_loss: 0.0051\n",
      "Epoch 00008: val_loss improved from 0.01459 to 0.01424, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_2665.h5\n",
      "333/333 [==============================] - 106s 319ms/step - loss: 0.0176 - semantic_0_loss: 0.0125 - semantic_1_loss: 0.0051 - val_loss: 0.0142 - val_semantic_0_loss: 0.0100 - val_semantic_1_loss: 0.0043\n",
      "Epoch 9/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0173 - semantic_0_loss: 0.0123 - semantic_1_loss: 0.0050\n",
      "Epoch 00009: val_loss did not improve from 0.01424\n",
      "333/333 [==============================] - 101s 304ms/step - loss: 0.0173 - semantic_0_loss: 0.0123 - semantic_1_loss: 0.0050 - val_loss: 0.0143 - val_semantic_0_loss: 0.0100 - val_semantic_1_loss: 0.0043\n",
      "Epoch 10/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0169 - semantic_0_loss: 0.0120 - semantic_1_loss: 0.0050\n",
      "Epoch 00010: val_loss did not improve from 0.01424\n",
      "333/333 [==============================] - 100s 302ms/step - loss: 0.0169 - semantic_0_loss: 0.0120 - semantic_1_loss: 0.0050 - val_loss: 0.1237 - val_semantic_0_loss: 0.1137 - val_semantic_1_loss: 0.0100\n",
      "Epoch 11/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0166 - semantic_0_loss: 0.0117 - semantic_1_loss: 0.0049\n",
      "Epoch 00011: val_loss improved from 0.01424 to 0.01392, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_2665.h5\n",
      "333/333 [==============================] - 106s 318ms/step - loss: 0.0166 - semantic_0_loss: 0.0117 - semantic_1_loss: 0.0049 - val_loss: 0.0139 - val_semantic_0_loss: 0.0097 - val_semantic_1_loss: 0.0042\n",
      "Epoch 12/100\n",
      "177/333 [==============>...............] - ETA: 33s - loss: 0.0164 - semantic_0_loss: 0.0115 - semantic_1_loss: 0.0049"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332/333 [============================>.] - ETA: 0s - loss: 0.0156 - semantic_0_loss: 0.0109 - semantic_1_loss: 0.0047\n",
      "Epoch 00022: val_loss did not improve from 0.01314\n",
      "333/333 [==============================] - 99s 298ms/step - loss: 0.0156 - semantic_0_loss: 0.0109 - semantic_1_loss: 0.0047 - val_loss: 0.0171 - val_semantic_0_loss: 0.0130 - val_semantic_1_loss: 0.0041\n",
      "Epoch 23/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0154 - semantic_0_loss: 0.0108 - semantic_1_loss: 0.0047\n",
      "Epoch 00023: val_loss did not improve from 0.01314\n",
      "333/333 [==============================] - 99s 297ms/step - loss: 0.0154 - semantic_0_loss: 0.0108 - semantic_1_loss: 0.0047 - val_loss: 0.0133 - val_semantic_0_loss: 0.0092 - val_semantic_1_loss: 0.0040\n",
      "Epoch 24/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0153 - semantic_0_loss: 0.0106 - semantic_1_loss: 0.0047\n",
      "Epoch 00024: val_loss improved from 0.01314 to 0.01312, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_2665.h5\n",
      "333/333 [==============================] - 104s 311ms/step - loss: 0.0153 - semantic_0_loss: 0.0106 - semantic_1_loss: 0.0047 - val_loss: 0.0131 - val_semantic_0_loss: 0.0091 - val_semantic_1_loss: 0.0040\n",
      "Epoch 25/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0152 - semantic_0_loss: 0.0106 - semantic_1_loss: 0.0046\n",
      "Epoch 00025: val_loss did not improve from 0.01312\n",
      "333/333 [==============================] - 99s 298ms/step - loss: 0.0152 - semantic_0_loss: 0.0106 - semantic_1_loss: 0.0046 - val_loss: 0.0132 - val_semantic_0_loss: 0.0092 - val_semantic_1_loss: 0.0040\n",
      "Epoch 26/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0154 - semantic_0_loss: 0.0107 - semantic_1_loss: 0.0047\n",
      "Epoch 00026: val_loss improved from 0.01312 to 0.01290, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_2665.h5\n",
      "333/333 [==============================] - 104s 312ms/step - loss: 0.0154 - semantic_0_loss: 0.0107 - semantic_1_loss: 0.0047 - val_loss: 0.0129 - val_semantic_0_loss: 0.0090 - val_semantic_1_loss: 0.0039\n",
      "Epoch 27/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0151 - semantic_0_loss: 0.0105 - semantic_1_loss: 0.0046\n",
      "Epoch 00027: val_loss did not improve from 0.01290\n",
      "333/333 [==============================] - 99s 297ms/step - loss: 0.0151 - semantic_0_loss: 0.0105 - semantic_1_loss: 0.0046 - val_loss: 0.0181 - val_semantic_0_loss: 0.0117 - val_semantic_1_loss: 0.0064\n",
      "Epoch 28/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0150 - semantic_0_loss: 0.0104 - semantic_1_loss: 0.0046\n",
      "Epoch 00028: val_loss did not improve from 0.01290\n",
      "333/333 [==============================] - 98s 294ms/step - loss: 0.0150 - semantic_0_loss: 0.0104 - semantic_1_loss: 0.0046 - val_loss: 0.0155 - val_semantic_0_loss: 0.0113 - val_semantic_1_loss: 0.0042\n",
      "Epoch 29/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0151 - semantic_0_loss: 0.0105 - semantic_1_loss: 0.0046\n",
      "Epoch 00029: val_loss improved from 0.01290 to 0.01287, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_2665.h5\n",
      "333/333 [==============================] - 103s 311ms/step - loss: 0.0151 - semantic_0_loss: 0.0105 - semantic_1_loss: 0.0046 - val_loss: 0.0129 - val_semantic_0_loss: 0.0089 - val_semantic_1_loss: 0.0040\n",
      "Epoch 30/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0151 - semantic_0_loss: 0.0105 - semantic_1_loss: 0.0046\n",
      "Epoch 00030: val_loss did not improve from 0.01287\n",
      "333/333 [==============================] - 99s 296ms/step - loss: 0.0151 - semantic_0_loss: 0.0105 - semantic_1_loss: 0.0046 - val_loss: 0.0129 - val_semantic_0_loss: 0.0089 - val_semantic_1_loss: 0.0039\n",
      "Epoch 31/100\n",
      "254/333 [=====================>........] - ETA: 16s - loss: 0.0151 - semantic_0_loss: 0.0105 - semantic_1_loss: 0.0046"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332/333 [============================>.] - ETA: 0s - loss: 0.0145 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0045\n",
      "Epoch 00042: val_loss did not improve from 0.01249\n",
      "333/333 [==============================] - 98s 295ms/step - loss: 0.0146 - semantic_0_loss: 0.0101 - semantic_1_loss: 0.0045 - val_loss: 0.0127 - val_semantic_0_loss: 0.0088 - val_semantic_1_loss: 0.0039\n",
      "Epoch 43/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0144 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0045\n",
      "Epoch 00043: val_loss did not improve from 0.01249\n",
      "333/333 [==============================] - 98s 293ms/step - loss: 0.0144 - semantic_0_loss: 0.0100 - semantic_1_loss: 0.0045 - val_loss: 0.0129 - val_semantic_0_loss: 0.0090 - val_semantic_1_loss: 0.0039\n",
      "Epoch 44/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0144 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0045\n",
      "Epoch 00044: val_loss did not improve from 0.01249\n",
      "333/333 [==============================] - 99s 296ms/step - loss: 0.0144 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0045 - val_loss: 0.0142 - val_semantic_0_loss: 0.0103 - val_semantic_1_loss: 0.0039\n",
      "Epoch 45/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0144 - semantic_0_loss: 0.0100 - semantic_1_loss: 0.0044\n",
      "Epoch 00045: val_loss improved from 0.01249 to 0.01247, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_2665.h5\n",
      "333/333 [==============================] - 103s 310ms/step - loss: 0.0144 - semantic_0_loss: 0.0100 - semantic_1_loss: 0.0044 - val_loss: 0.0125 - val_semantic_0_loss: 0.0086 - val_semantic_1_loss: 0.0039\n",
      "Epoch 46/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0143 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044\n",
      "Epoch 00046: val_loss did not improve from 0.01247\n",
      "333/333 [==============================] - 98s 293ms/step - loss: 0.0143 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044 - val_loss: 0.0234 - val_semantic_0_loss: 0.0188 - val_semantic_1_loss: 0.0046\n",
      "Epoch 47/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0143 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044\n",
      "Epoch 00047: val_loss did not improve from 0.01247\n",
      "333/333 [==============================] - 97s 292ms/step - loss: 0.0143 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044 - val_loss: 0.0156 - val_semantic_0_loss: 0.0117 - val_semantic_1_loss: 0.0039\n",
      "Epoch 48/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0143 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044\n",
      "Epoch 00048: val_loss did not improve from 0.01247\n",
      "333/333 [==============================] - 98s 294ms/step - loss: 0.0143 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044 - val_loss: 0.0660 - val_semantic_0_loss: 0.0617 - val_semantic_1_loss: 0.0043\n",
      "Epoch 49/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0143 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044\n",
      "Epoch 00049: val_loss did not improve from 0.01247\n",
      "333/333 [==============================] - 97s 292ms/step - loss: 0.0143 - semantic_0_loss: 0.0099 - semantic_1_loss: 0.0044 - val_loss: 0.0130 - val_semantic_0_loss: 0.0091 - val_semantic_1_loss: 0.0039\n",
      "Epoch 50/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0143 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0044\n",
      "Epoch 00050: val_loss did not improve from 0.01247\n",
      "333/333 [==============================] - 97s 293ms/step - loss: 0.0142 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0044 - val_loss: 0.0126 - val_semantic_0_loss: 0.0087 - val_semantic_1_loss: 0.0039\n",
      "Epoch 51/100\n",
      "  3/333 [..............................] - ETA: 51s - loss: 0.0146 - semantic_0_loss: 0.0104 - semantic_1_loss: 0.0042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332/333 [============================>.] - ETA: 0s - loss: 0.0142 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0044\n",
      "Epoch 00061: val_loss did not improve from 0.01247\n",
      "333/333 [==============================] - 98s 293ms/step - loss: 0.0142 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0044 - val_loss: 0.1509 - val_semantic_0_loss: 0.1470 - val_semantic_1_loss: 0.0039\n",
      "Epoch 62/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043\n",
      "Epoch 00062: val_loss improved from 0.01247 to 0.01242, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_2665.h5\n",
      "333/333 [==============================] - 104s 313ms/step - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043 - val_loss: 0.0124 - val_semantic_0_loss: 0.0086 - val_semantic_1_loss: 0.0039\n",
      "Epoch 63/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0141 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0043\n",
      "Epoch 00063: val_loss did not improve from 0.01242\n",
      "333/333 [==============================] - 99s 298ms/step - loss: 0.0141 - semantic_0_loss: 0.0098 - semantic_1_loss: 0.0043 - val_loss: 0.0156 - val_semantic_0_loss: 0.0111 - val_semantic_1_loss: 0.0045\n",
      "Epoch 64/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0139 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043\n",
      "Epoch 00064: val_loss did not improve from 0.01242\n",
      "333/333 [==============================] - 100s 301ms/step - loss: 0.0139 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043 - val_loss: 0.0847 - val_semantic_0_loss: 0.0808 - val_semantic_1_loss: 0.0039\n",
      "Epoch 65/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0140 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0044\n",
      "Epoch 00065: val_loss did not improve from 0.01242\n",
      "333/333 [==============================] - 102s 305ms/step - loss: 0.0140 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0044 - val_loss: 0.1176 - val_semantic_0_loss: 0.1137 - val_semantic_1_loss: 0.0040\n",
      "Epoch 66/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043\n",
      "Epoch 00066: val_loss did not improve from 0.01242\n",
      "333/333 [==============================] - 103s 310ms/step - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043 - val_loss: 0.2195 - val_semantic_0_loss: 0.2138 - val_semantic_1_loss: 0.0057\n",
      "Epoch 67/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0140 - semantic_0_loss: 0.0097 - semantic_1_loss: 0.0043\n",
      "Epoch 00067: val_loss improved from 0.01242 to 0.01230, saving model to /data/analyses/size_benchmarking/20201018_multiplex_seed_2__subset_2665.h5\n",
      "333/333 [==============================] - 108s 325ms/step - loss: 0.0140 - semantic_0_loss: 0.0097 - semantic_1_loss: 0.0043 - val_loss: 0.0123 - val_semantic_0_loss: 0.0085 - val_semantic_1_loss: 0.0038\n",
      "Epoch 68/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043\n",
      "Epoch 00068: val_loss did not improve from 0.01230\n",
      "333/333 [==============================] - 104s 314ms/step - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043 - val_loss: 0.0233 - val_semantic_0_loss: 0.0181 - val_semantic_1_loss: 0.0052\n",
      "Epoch 69/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043\n",
      "Epoch 00069: val_loss did not improve from 0.01230\n",
      "333/333 [==============================] - 104s 312ms/step - loss: 0.0139 - semantic_0_loss: 0.0096 - semantic_1_loss: 0.0043 - val_loss: 0.1637 - val_semantic_0_loss: 0.1593 - val_semantic_1_loss: 0.0044\n",
      "Epoch 70/100\n",
      "104/333 [========>.....................] - ETA: 48s - loss: 0.0138 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332/333 [============================>.] - ETA: 0s - loss: 0.0136 - semantic_0_loss: 0.0093 - semantic_1_loss: 0.0043\n",
      "Epoch 00080: val_loss did not improve from 0.01221\n",
      "333/333 [==============================] - 106s 318ms/step - loss: 0.0136 - semantic_0_loss: 0.0093 - semantic_1_loss: 0.0043 - val_loss: 0.0143 - val_semantic_0_loss: 0.0098 - val_semantic_1_loss: 0.0045\n",
      "Epoch 81/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0137 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0043\n",
      "Epoch 00081: val_loss did not improve from 0.01221\n",
      "333/333 [==============================] - 107s 321ms/step - loss: 0.0137 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0043 - val_loss: 0.1713 - val_semantic_0_loss: 0.1670 - val_semantic_1_loss: 0.0043\n",
      "Epoch 82/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0137 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0043\n",
      "Epoch 00082: val_loss did not improve from 0.01221\n",
      "333/333 [==============================] - 106s 319ms/step - loss: 0.0137 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0043 - val_loss: 0.0165 - val_semantic_0_loss: 0.0113 - val_semantic_1_loss: 0.0052\n",
      "Epoch 83/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0137 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043\n",
      "Epoch 00083: val_loss did not improve from 0.01221\n",
      "333/333 [==============================] - 106s 319ms/step - loss: 0.0137 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043 - val_loss: 0.0133 - val_semantic_0_loss: 0.0089 - val_semantic_1_loss: 0.0044\n",
      "Epoch 84/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0135 - semantic_0_loss: 0.0093 - semantic_1_loss: 0.0042\n",
      "Epoch 00084: val_loss did not improve from 0.01221\n",
      "333/333 [==============================] - 105s 315ms/step - loss: 0.0135 - semantic_0_loss: 0.0093 - semantic_1_loss: 0.0042 - val_loss: 0.0534 - val_semantic_0_loss: 0.0478 - val_semantic_1_loss: 0.0056\n",
      "Epoch 85/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0137 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043\n",
      "Epoch 00085: val_loss did not improve from 0.01221\n",
      "333/333 [==============================] - 106s 317ms/step - loss: 0.0137 - semantic_0_loss: 0.0095 - semantic_1_loss: 0.0043 - val_loss: 0.0152 - val_semantic_0_loss: 0.0113 - val_semantic_1_loss: 0.0039\n",
      "Epoch 86/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0137 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0043\n",
      "Epoch 00086: val_loss did not improve from 0.01221\n",
      "333/333 [==============================] - 105s 315ms/step - loss: 0.0137 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0043 - val_loss: 0.0125 - val_semantic_0_loss: 0.0085 - val_semantic_1_loss: 0.0040\n",
      "Epoch 87/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0135 - semantic_0_loss: 0.0092 - semantic_1_loss: 0.0043\n",
      "Epoch 00087: val_loss did not improve from 0.01221\n",
      "333/333 [==============================] - 105s 316ms/step - loss: 0.0135 - semantic_0_loss: 0.0092 - semantic_1_loss: 0.0043 - val_loss: 0.0262 - val_semantic_0_loss: 0.0223 - val_semantic_1_loss: 0.0038\n",
      "Epoch 88/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0137 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0043\n",
      "Epoch 00088: val_loss did not improve from 0.01221\n",
      "333/333 [==============================] - 106s 317ms/step - loss: 0.0137 - semantic_0_loss: 0.0094 - semantic_1_loss: 0.0043 - val_loss: 0.0151 - val_semantic_0_loss: 0.0112 - val_semantic_1_loss: 0.0039\n",
      "Epoch 89/100\n",
      "245/333 [=====================>........] - ETA: 19s - loss: 0.0135 - semantic_0_loss: 0.0093 - semantic_1_loss: 0.0042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332/333 [============================>.] - ETA: 0s - loss: 0.0134 - semantic_0_loss: 0.0092 - semantic_1_loss: 0.0042\n",
      "Epoch 00099: val_loss did not improve from 0.01210\n",
      "333/333 [==============================] - 107s 322ms/step - loss: 0.0134 - semantic_0_loss: 0.0092 - semantic_1_loss: 0.0042 - val_loss: 0.0130 - val_semantic_0_loss: 0.0087 - val_semantic_1_loss: 0.0043\n",
      "Epoch 100/100\n",
      "332/333 [============================>.] - ETA: 0s - loss: 0.0135 - semantic_0_loss: 0.0093 - semantic_1_loss: 0.0042\n",
      "Epoch 00100: val_loss did not improve from 0.01210\n",
      "333/333 [==============================] - 107s 321ms/step - loss: 0.0135 - semantic_0_loss: 0.0093 - semantic_1_loss: 0.0042 - val_loss: 0.0127 - val_semantic_0_loss: 0.0085 - val_semantic_1_loss: 0.0042\n"
     ]
    }
   ],
   "source": [
    "from deepcell.model_zoo.panopticnet import PanopticNet\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "from deepcell import image_generators\n",
    "from deepcell.utils import train_utils\n",
    "from tensorflow.python.keras.losses import MSE\n",
    "from deepcell import losses\n",
    "from deepcell.utils.train_utils import get_callbacks\n",
    "from deepcell.utils.train_utils import count_gpus\n",
    "\n",
    "for idx in range(6, len(train_keys)):\n",
    "    print(\"Training model for {}\".format(train_keys[idx]))\n",
    "    train_key, val_key = train_keys[idx], val_keys[idx]\n",
    "    \n",
    "    # initialize new model\n",
    "    new_model = PanopticNet(\n",
    "        backbone='resnet50',\n",
    "        input_shape=(256, 256, 2),\n",
    "        norm_method=None,\n",
    "        num_semantic_heads=2,\n",
    "        num_semantic_classes=[1, 3], # inner distance, pixelwise\n",
    "        location=True,  # should always be true\n",
    "        include_top=True)\n",
    "    \n",
    "    \n",
    "    X_train = train_dict[train_key].item()['X']\n",
    "    X_train = multiplex_preprocess(X_train)\n",
    "    y_train = train_dict[train_key].item()['y']\n",
    "    print(\"X_train shape is {}, y_train shape is {}\".format(X_train.shape, y_train.shape))\n",
    "    \n",
    "    \n",
    "    X_val = val_dict[val_key].item()['X']\n",
    "    X_val = multiplex_preprocess(X_val)\n",
    "    y_val = val_dict[val_key].item()['y']\n",
    "    print(\"X_val shape is {}, y_val shape is {}\".format(X_val.shape, y_val.shape))\n",
    "        \n",
    "    # set up training parameters\n",
    "    model_name = npz_name + '_subset_' + train_key\n",
    "    n_epoch = 100  # Number of training epochs\n",
    "\n",
    "    optimizer = Adam(lr=1e-4, clipnorm=0.001)\n",
    "    lr_sched = rate_scheduler(lr=1e-4, decay=0.99)\n",
    "\n",
    "    batch_size = 8\n",
    "\n",
    "    min_objects = 0  # throw out images with fewer than this many objects\n",
    "    seed=0\n",
    "    print(\"Model name is {}\".format(model_name))\n",
    "    \n",
    "    # create augmented dataset\n",
    "    datagen = image_generators.CroppingDataGenerator(\n",
    "        rotation_range=180,\n",
    "        shear_range=0,\n",
    "        zoom_range=(0.7, 1/0.7),\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        crop_size=(256, 256),\n",
    "        float_dtype='float16',\n",
    "        int_dtype='int16')\n",
    "\n",
    "    datagen_val = image_generators.SemanticDataGenerator(\n",
    "        rotation_range=0,\n",
    "        shear_range=0,\n",
    "        zoom_range=0,\n",
    "        horizontal_flip=0,\n",
    "        vertical_flip=0,\n",
    "        float_dtype='float16',\n",
    "        int_dtype='int16')\n",
    "\n",
    "    train_data = datagen.flow(\n",
    "        {'X': X_train, 'y': y_train},\n",
    "        seed=seed,\n",
    "        transforms=['inner-distance', 'pixelwise'],\n",
    "        transforms_kwargs={'pixelwise':{'dilation_radius': 1}, \n",
    "                          'inner-distance': {'erosion_width': 1, 'alpha': 'auto'}},\n",
    "        min_objects=min_objects,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    val_data = datagen_val.flow(\n",
    "        {'X': X_val, 'y': y_val},\n",
    "        seed=seed,\n",
    "        transforms=['inner-distance', 'pixelwise'],\n",
    "        transforms_kwargs={'pixelwise':{'dilation_radius': 1},\n",
    "                          'inner-distance': {'erosion_width': 1, 'alpha': 'auto'}},\n",
    "        min_objects=min_objects,\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    print('generators created')\n",
    "    \n",
    "    # set up losses\n",
    "    def semantic_loss(n_classes):\n",
    "        def _semantic_loss(y_pred, y_true):\n",
    "            if n_classes > 1:\n",
    "                return 0.01 * losses.weighted_categorical_crossentropy(\n",
    "                    y_pred, y_true, n_classes=n_classes)\n",
    "            return MSE(y_pred, y_true)\n",
    "        return _semantic_loss\n",
    "\n",
    "\n",
    "    loss = {}\n",
    "\n",
    "    # Give losses for all of the semantic heads\n",
    "    for layer in new_model.layers:\n",
    "        if layer.name.startswith('semantic_'):\n",
    "            n_classes = layer.output_shape[-1]\n",
    "            loss[layer.name] = semantic_loss(n_classes)\n",
    "            \n",
    "    # compile model\n",
    "    new_model.compile(loss=loss, optimizer=optimizer)\n",
    "    \n",
    "    # train model\n",
    "    model_path = os.path.join(MODEL_DIR, '{}.h5'.format(model_name))\n",
    "    loss_path = os.path.join(MODEL_DIR, '{}.npz'.format(model_name))\n",
    "\n",
    "    num_gpus = count_gpus()\n",
    "\n",
    "    print('Training on', num_gpus, 'GPUs.')\n",
    "\n",
    "    train_callbacks = get_callbacks(\n",
    "        model_path,\n",
    "        lr_sched=lr_sched,\n",
    "        #tensorboard_log_dir=LOG_DIR,\n",
    "        save_weights_only=num_gpus >= 2,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "\n",
    "    loss_history = new_model.fit_generator(\n",
    "        train_data,\n",
    "        steps_per_epoch=333,\n",
    "        epochs=n_epoch,\n",
    "        validation_data=val_data,\n",
    "        validation_steps=val_data.y.shape[0] // batch_size,\n",
    "        callbacks=train_callbacks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
