{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the deepcell-tf documentation: https://deepcell.readthedocs.io/.\n",
    "\n",
    "# Training a cell tracking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow_addons.optimizers import RectifiedAdam\n",
    "import yaml\n",
    "\n",
    "from deepcell.data.tracking import Track, random_rotate, random_translate, temporal_slice\n",
    "from deepcell.model_zoo.tracking import GNNTrackingModel\n",
    "from deepcell.utils.tfrecord_utils import get_tracking_dataset, write_tracking_dataset_to_tfr\n",
    "from deepcell.utils.train_utils import count_gpus, rate_scheduler\n",
    "from deepcell_toolbox.metrics import Metrics\n",
    "from deepcell_tracking import CellTracker\n",
    "from deepcell_tracking.metrics import benchmark_tracking_performance, calculate_summary_stats\n",
    "from deepcell_tracking.trk_io import load_trks\n",
    "from deepcell_tracking.utils import get_max_cells, is_valid_lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DynamicNuclearNet tracking dataset can be downloaded from https://datasets.deepcell.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please change these file paths to match your file system.\n",
    "data_dir = 'data'\n",
    "\n",
    "inf_model_path = \"NuclearTrackingInf\"\n",
    "ne_model_path = \"NuclearTrackingNE\"\n",
    "metrics_path = \"train-metrics.yaml\"\n",
    "log_path = \"train_log.csv\"\n",
    "\n",
    "prediction_dir = 'output'\n",
    "# Check that prediction directory exists and make if needed\n",
    "    if not os.path.exists(prediction_dir):\n",
    "        os.makedirs(prediction_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare the data for training\n",
    "\n",
    "Tracked data are stored as `.trks` files. These files include images and lineage data in np.arrays. To manipulate `.trks` files, use `deepcell_tracking.trk_io.load_trks` and `deepcell_tracking.trk_io.save_trks`.\n",
    "\n",
    "To facilitate training, we transform each movie's image and lineage data into a `Track` object.\n",
    "`Tracks` help to encapsulate all of the feature creation from the movie, including:\n",
    "\n",
    "* Appearances: `(num_frames, num_objects, 32, 32, 1)`\n",
    "* Morphologies: `(num_frames, num_objects, 32, 32, 3)`\n",
    "* Centroids: `(num_frames, num_objects, 2)`\n",
    "* Normalized Adjacency Matrix: `(num_frames, num_objects, num_objects, 3)`\n",
    "* Temporal Adjacency Matrix (comparing across frames): `(num_frames - 1, num_objects, num_objects, 3)`\n",
    "\n",
    "Each `Track` is then saved as a tfrecord file in order to load data from disk during training and reduce the total memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appearance_dim = 32\n",
    "distance_threshold = 64\n",
    "crop_mode = \"resize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell may take ~20 minutes to run\n",
    "train_trks = load_trks(os.path.join(data_dir, \"train.trks\"))\n",
    "val_trks = load_trks(os.path.join(data_dir, \"val.trks\"))\n",
    "\n",
    "max_cells = max([get_max_cells(train_trks[\"y\"]), get_max_cells(val_trks[\"y\"])])\n",
    "\n",
    "for split, trks in zip({\"train\", \"val\"}, [train_trks, val_trks]):\n",
    "    print(f\"Preparing {split} as tf record\")\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        tracks = Track(\n",
    "            tracked_data=trks,\n",
    "            appearance_dim=appearance_dim,\n",
    "            distance_threshold=distance_threshold,\n",
    "            crop_mode=crop_mode,\n",
    "        )\n",
    "\n",
    "        write_tracking_dataset_to_tfr(\n",
    "            tracks, target_max_cells=max_cells, filename=split\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0  \n",
    "batch_size = 8\n",
    "track_length = 8  # Number of frames per track object\n",
    "n_layers = 1  # Number of graph convolution layers\n",
    "n_filters = 64\n",
    "encoder_dim = 64\n",
    "embedding_dim = 64\n",
    "graph_layer = \"gat\"\n",
    "epochs = 50\n",
    "steps_per_epoch = 1000\n",
    "validation_steps = 200\n",
    "rotation_range = 180\n",
    "translation_range = 512\n",
    "buffer_size = 128\n",
    "lr = 1e-3\n",
    "norm_layer = \"batch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load TFRecord Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation functions\n",
    "def sample(X, y):\n",
    "    return temporal_slice(X, y, track_length=track_length)\n",
    "\n",
    "def rotate(X, y):\n",
    "    return random_rotate(X, y, rotation_range=rotation_range)\n",
    "\n",
    "def translate(X, y):\n",
    "    return random_translate(X, y, range=translation_range)\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    train_data = get_tracking_dataset(\"train\")\n",
    "    train_data = train_data.shuffle(buffer_size, seed=seed).repeat()\n",
    "    train_data = train_data.map(sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_data = train_data.map(rotate, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_data = train_data.map(translate, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_data = train_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    val_data = get_tracking_dataset(\"val\")\n",
    "    val_data = val_data.shuffle(buffer_size, seed=seed).repeat()\n",
    "    val_data = val_data.map(sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_data = val_data.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "max_cells = list(train_data.take(1))[0][0][\"appearances\"].shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_flatten(y_true, y_pred):\n",
    "    n_classes = tf.shape(y_true)[-1]\n",
    "    new_shape = [-1, n_classes]\n",
    "    y_true = tf.reshape(y_true, new_shape)\n",
    "    y_pred = tf.reshape(y_pred, new_shape)\n",
    "\n",
    "    # Mask out the padded cells\n",
    "    y_true_reduced = tf.reduce_sum(y_true, axis=-1)\n",
    "    good_loc = tf.where(y_true_reduced == 1)[:, 0]\n",
    "\n",
    "    y_true = tf.gather(y_true, good_loc, axis=0)\n",
    "    y_pred = tf.gather(y_pred, good_loc, axis=0)\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "class Recall(tf.keras.metrics.Recall):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "        super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "class Precision(tf.keras.metrics.Precision):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "        super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true, y_pred = filter_and_flatten(y_true, y_pred)\n",
    "    return deepcell.losses.weighted_categorical_crossentropy(\n",
    "        y_true, y_pred, n_classes=tf.shape(y_true)[-1], axis=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "with strategy.scope():\n",
    "    model = GNNTrackingModel(\n",
    "        max_cells=max_cells,\n",
    "        graph_layer=graph_layer,\n",
    "        track_length=track_length,\n",
    "        n_filters=n_filters,\n",
    "        embedding_dim=embedding_dim,\n",
    "        encoder_dim=encoder_dim,\n",
    "        n_layers=n_layers,\n",
    "        norm_layer=norm_layer,\n",
    "    )\n",
    "\n",
    "    loss = {\"temporal_adj_matrices\": loss_function}\n",
    "\n",
    "    optimizer = RectifiedAdam(learning_rate=lr, clipnorm=0.001)\n",
    "\n",
    "    training_metrics = [\n",
    "        Recall(class_id=0, name=\"same_recall\"),\n",
    "        Recall(class_id=1, name=\"different_recall\"),\n",
    "        Recall(class_id=2, name=\"daughter_recall\"),\n",
    "        Precision(class_id=0, name=\"same_precision\"),\n",
    "        Precision(class_id=1, name=\"different_precision\"),\n",
    "        Precision(class_id=2, name=\"daughter_precision\"),\n",
    "    ]\n",
    "\n",
    "    model.training_model.compile(\n",
    "        loss=loss, optimizer=optimizer, metrics=training_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear clutter from previous TensorFlow graphs.\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "monitor = \"val_loss\"\n",
    "\n",
    "csv_logger = CSVLogger(train_log_path)\n",
    "\n",
    "# Create callbacks for early stopping and pruning.\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.LearningRateScheduler(rate_scheduler(lr=lr, decay=0.99)),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=monitor,\n",
    "        factor=0.1,\n",
    "        patience=5,\n",
    "        verbose=1,\n",
    "        mode=\"auto\",\n",
    "        min_delta=0.0001,\n",
    "        cooldown=0,\n",
    "        min_lr=0,\n",
    "    ),\n",
    "    csv_logger,\n",
    "]\n",
    "\n",
    "print(f\"Training on {count_gpus()} GPUs.\")\n",
    "\n",
    "# Train model.\n",
    "history = model.training_model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_data,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(\"Final\", monitor, \":\", history.history[monitor][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "model.inference_model.save(inf_model_path, include_optimizer=False, overwrite=True)\n",
    "model.neighborhood_encoder.save(\n",
    "    ne_model_path, include_optimizer=False, overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record training metrics\n",
    "all_metrics = {\n",
    "    \"metrics\": {\"training\": {k: float(v[-1]) for k, v in history.history.items()}}\n",
    "}\n",
    "\n",
    "# save a metadata.yaml file in the saved model directory\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    yaml.dump(all_metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set tracking parameters and `CellTracker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "death = 0.99\n",
    "birth = 0.99\n",
    "division = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_trks(os.path.join(data_dir, \"test.trks\"))\n",
    "X_test = test_data[\"X\"]\n",
    "y_test = test_data[\"y\"]\n",
    "lineages_test = test_data[\"lineages\"]\n",
    "\n",
    "# Load metadata array\n",
    "with np.load(os.path.join(data_dir, \"data-source.npz\"), allow_pickle=True) as data:\n",
    "    meta = data[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "exp_metrics = {}\n",
    "bad_batches = []\n",
    "for b in range(len(X_test)):\n",
    "    # currently NOT saving any recall/precision information\n",
    "    gt_path = os.path.join(prediction_dir, f\"{b}-gt.trk\")\n",
    "    res_path = os.path.join(prediction_dir, f\"{b}-res.trk\")\n",
    "\n",
    "    # Check that lineage is valid before proceeding\n",
    "    if not is_valid_lineage(y_test[b], lineages_test[b]):\n",
    "        bad_batches.append(b)\n",
    "        continue\n",
    "\n",
    "    frames = find_frames_with_objects(y_test[b])\n",
    "\n",
    "    tracker = CellTracker(\n",
    "        movie=X_test[b][frames],\n",
    "        annotation=y_test[b][frames],\n",
    "        track_length=track_length,\n",
    "        neighborhood_encoder=ne_model,\n",
    "        tracking_model=inf_model,\n",
    "        death=death,\n",
    "        birth=birth,\n",
    "        division=division,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        tracker.track_cells()\n",
    "    except Exception as err:\n",
    "        print(\n",
    "            \"Failed to track batch {} due to {}: {}\".format(\n",
    "                b, err.__class__.__name__, err\n",
    "            )\n",
    "        )\n",
    "        bad_batches.append(b)\n",
    "        continue\n",
    "\n",
    "    tracker.dump(res_path)\n",
    "\n",
    "    gt = {\n",
    "        \"X\": X_test[b][frames],\n",
    "        \"y_tracked\": y_test[b][frames],\n",
    "        \"tracks\": lineages_test[b],\n",
    "    }\n",
    "\n",
    "    tracker.dump(filename=gt_path, track_review_dict=gt)\n",
    "\n",
    "    results = benchmark_tracking_performance(\n",
    "        gt_path, res_path, threshold=iou_thresh\n",
    "    )\n",
    "\n",
    "    exp = meta[b, 1]  # Grab the experiment column from metadata\n",
    "    tmp_exp = exp_metrics.get(exp, {})\n",
    "\n",
    "    for k in results:\n",
    "        if k in metrics:\n",
    "            metrics[k] += results[k]\n",
    "        else:\n",
    "            metrics[k] = results[k]\n",
    "\n",
    "        if k in tmp_exp:\n",
    "            tmp_exp[k] += results[k]\n",
    "        else:\n",
    "            tmp_exp[k] = results[k]\n",
    "\n",
    "    exp_metrics[exp] = tmp_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary stats for each set of metrics\n",
    "tmp_metrics = metrics.copy()\n",
    "del tmp_metrics[\"mismatch_division\"]\n",
    "summary = calculate_summary_stats(**tmp_metrics, n_digits=3)\n",
    "metrics = {**metrics, **summary}\n",
    "\n",
    "for exp, m in exp_metrics.items():\n",
    "    tmp_m = m.copy()\n",
    "    del tmp_m[\"mismatch_division\"]\n",
    "    summary = calculate_summary_stats(**tmp_m, n_digits=3)\n",
    "    exp_metrics[exp] = {**m, **summary}\n",
    "\n",
    "# save a metadata.yaml file in the saved model directory\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    yaml.dump(all_metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cfb52fc353dd44a1d9596f9524249f9f215ec12b042508c5a2004684bb3403a0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
