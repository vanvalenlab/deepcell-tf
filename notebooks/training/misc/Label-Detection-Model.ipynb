{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Type Detection\n",
    "Train a model that can predict the label captured in a representitive image: phase, nuclear, fluorescent cytoplasm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import deepcell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some global constants and shared filepaths\n",
    "\n",
    "SEED = 213  # random seed for splitting data into train/test\n",
    "\n",
    "ROOT_DIR = '/data'  # TODO: Change this! Usually a mounted volume\n",
    "DATA_DIR = os.path.expanduser(os.path.join('~', '.keras', 'datasets'))\n",
    "MODEL_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'models'))\n",
    "LOG_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'logs'))\n",
    "\n",
    "MODEL_NAME = 'LabelDetectionModel'\n",
    "\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_NAME + '.h5')\n",
    "\n",
    "# create directories if they do not exist\n",
    "for d in (MODEL_DIR, LOG_DIR):\n",
    "    try:\n",
    "        os.makedirs(d)\n",
    "    except OSError as exc:  # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data\n",
    "\n",
    "Download data for nuclear, brightfield and fluorescent cytoplasm from `deepcell.datasets` and combine the data into a single training dataset.\n",
    "\n",
    "The labels for each type of data are as follows:\n",
    "\n",
    "- Nuclear = 0\n",
    "- Phase = 1\n",
    "- Fluorescent Cytoplasm = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, download the data from deepcell.datasets\n",
    "import deepcell.datasets\n",
    "\n",
    "# nuclear data (label type 0)\n",
    "(X_train, y_train), (X_test, y_test) = deepcell.datasets.hela_s3.load_data(seed=SEED)\n",
    "nuclear_train = {'X': X_train, 'y': y_train}\n",
    "nuclear_test = {'X': X_test, 'y': y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brightfield phase data (label type 1)\n",
    "(X_train, y_train), (X_test, y_test) = deepcell.datasets.phase.HeLa_S3.load_data(seed=SEED)\n",
    "brightfield_train = {'X': X_train, 'y': y_train}\n",
    "brightfield_test = {'X': X_test, 'y': y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flourescent cytoplasm data (label type 2)\n",
    "(X_train, y_train), (X_test, y_test) = deepcell.datasets.cytoplasm.hela_s3.load_data(seed=SEED)\n",
    "flourescent_train = {'X': X_train, 'y': y_train}\n",
    "flourescent_test = {'X': X_test, 'y': y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped feature data from (5760, 216, 256, 1) to (5760, 216, 216, 1)\n",
      "Reshaped training data from (5760, 216, 256, 1) to (5760, 216, 216, 1)\n",
      "Reshaped feature data from (1440, 216, 256, 1) to (1440, 216, 216, 1)\n",
      "Reshaped training data from (1440, 216, 256, 1) to (1440, 216, 216, 1)\n",
      "Reshaped feature data from (1651, 512, 512, 1) to (14859, 216, 216, 1)\n",
      "Reshaped training data from (1651, 512, 512, 1) to (14859, 216, 216, 1)\n",
      "Reshaped feature data from (413, 512, 512, 1) to (3717, 216, 216, 1)\n",
      "Reshaped training data from (413, 512, 512, 1) to (3717, 216, 216, 1)\n",
      "Reshaped feature data from (1651, 512, 512, 1) to (14859, 216, 216, 1)\n",
      "Reshaped training data from (1651, 512, 512, 1) to (14859, 216, 216, 1)\n",
      "Reshaped feature data from (413, 512, 512, 1) to (3717, 216, 216, 1)\n",
      "Reshaped training data from (413, 512, 512, 1) to (3717, 216, 216, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape each dataset to conform to the minimum size of 216\n",
    "from deepcell.utils.data_utils import reshape_matrix\n",
    "\n",
    "RESHAPE_SIZE = 216\n",
    "\n",
    "all_train = [nuclear_train, brightfield_train, flourescent_train]\n",
    "all_test = [nuclear_test, brightfield_test, flourescent_test]\n",
    "\n",
    "for train, test in zip(all_train, all_test):\n",
    "    train['X'], train['y'] = reshape_matrix(train['X'], train['y'], RESHAPE_SIZE)\n",
    "    test['X'], test['y'] = reshape_matrix(test['X'], test['y'], RESHAPE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack up our data as train and test\n",
    "\n",
    "def make_y(batch, i):\n",
    "    y = np.zeros((batch, 3))\n",
    "    y[:, i] = 1\n",
    "    return y\n",
    "\n",
    "X_train = np.vstack([\n",
    "    nuclear_train['X'],\n",
    "    brightfield_train['X'],\n",
    "    flourescent_train['X']\n",
    "])\n",
    "\n",
    "y_train = np.vstack([\n",
    "    make_y(nuclear_train['y'].shape[0], 0),\n",
    "    make_y(brightfield_train['y'].shape[0], 1),\n",
    "    make_y(flourescent_train['y'].shape[0], 2)\n",
    "])\n",
    "\n",
    "X_test = np.vstack([\n",
    "    nuclear_test['X'],\n",
    "    brightfield_test['X'],\n",
    "    flourescent_test['X']\n",
    "])\n",
    "\n",
    "y_test = np.vstack([\n",
    "    make_y(nuclear_test['y'].shape[0], 0),\n",
    "    make_y(brightfield_test['y'].shape[0], 1),\n",
    "    make_y(flourescent_test['y'].shape[0], 2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Label Detection Model\n",
    "\n",
    "We are using the LabelDetectionModel from `deepcell.applications`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 216, 216, 1)       0         \n",
      "_________________________________________________________________\n",
      "image_normalization2d (Image (None, 216, 216, 1)       3721      \n",
      "_________________________________________________________________\n",
      "tensor_product (TensorProduc (None, 216, 216, 3)       6         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 216, 216, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 216, 216, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 108, 108, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 108, 108, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 108, 108, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 54, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 54, 54, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 54, 54, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 54, 54, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 27, 27, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 27, 27, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 27, 27, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "tensor_product_1 (TensorProd (None, 1, 1, 256)         131328    \n",
      "_________________________________________________________________\n",
      "tensor_product_2 (TensorProd (None, 1, 1, 3)           771       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 14,850,514\n",
      "Trainable params: 14,846,793\n",
      "Non-trainable params: 3,721\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from deepcell.applications import LabelDetectionModel\n",
    "\n",
    "# set use_pretrained_weights=False to start training from scratch\n",
    "model = LabelDetectionModel(input_shape=X_train.shape[1:], use_pretrained_weights=False)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "from deepcell import losses\n",
    "\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    return losses.weighted_categorical_crossentropy(\n",
    "            y_true, y_pred,\n",
    "            n_classes=len(np.unique(y_train)))\n",
    "\n",
    "\n",
    "n_epoch = 20  # Number of training epochs\n",
    "lr = 1e-3\n",
    "optimizer = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "lr_sched = rate_scheduler(lr=lr, decay=0.9)\n",
    "batch_size = 32\n",
    "\n",
    "model.compile(optimizer, loss=loss_function, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Initialize data generators for training\n",
    "generator = ImageDataGenerator(\n",
    "    rotation_range=180,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=(0.5, 2))\n",
    "\n",
    "val_generator = ImageDataGenerator()  # No augmentation for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.8642 - acc: 0.7551\n",
      "Epoch 00001: val_loss improved from inf to 0.40321, saving model to /data/models/LabelDetectionModel.h5\n",
      "1108/1108 [==============================] - 185s 167ms/step - loss: 0.8639 - acc: 0.7552 - val_loss: 0.4032 - val_acc: 0.8833\n",
      "Epoch 2/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.3728 - acc: 0.8967\n",
      "Epoch 00002: val_loss improved from 0.40321 to 0.22479, saving model to /data/models/LabelDetectionModel.h5\n",
      "1108/1108 [==============================] - 177s 160ms/step - loss: 0.3728 - acc: 0.8968 - val_loss: 0.2248 - val_acc: 0.9457\n",
      "Epoch 3/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9414\n",
      "Epoch 00003: val_loss did not improve from 0.22479\n",
      "1108/1108 [==============================] - 177s 160ms/step - loss: 0.2346 - acc: 0.9414 - val_loss: 0.2909 - val_acc: 0.9351\n",
      "Epoch 4/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.1757 - acc: 0.9564\n",
      "Epoch 00004: val_loss did not improve from 0.22479\n",
      "1108/1108 [==============================] - 177s 159ms/step - loss: 0.1756 - acc: 0.9564 - val_loss: 0.2477 - val_acc: 0.9510\n",
      "Epoch 5/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9634\n",
      "Epoch 00005: val_loss improved from 0.22479 to 0.14946, saving model to /data/models/LabelDetectionModel.h5\n",
      "1108/1108 [==============================] - 177s 160ms/step - loss: 0.1542 - acc: 0.9634 - val_loss: 0.1495 - val_acc: 0.9673\n",
      "Epoch 6/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.9697\n",
      "Epoch 00006: val_loss improved from 0.14946 to 0.14592, saving model to /data/models/LabelDetectionModel.h5\n",
      "1108/1108 [==============================] - 177s 160ms/step - loss: 0.1295 - acc: 0.9698 - val_loss: 0.1459 - val_acc: 0.9666\n",
      "Epoch 7/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.1187 - acc: 0.9727\n",
      "Epoch 00007: val_loss did not improve from 0.14592\n",
      "1108/1108 [==============================] - 177s 160ms/step - loss: 0.1187 - acc: 0.9727 - val_loss: 0.2314 - val_acc: 0.9506\n",
      "Epoch 8/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9741\n",
      "Epoch 00008: val_loss did not improve from 0.14592\n",
      "1108/1108 [==============================] - 176s 159ms/step - loss: 0.1078 - acc: 0.9741 - val_loss: 0.1462 - val_acc: 0.9694\n",
      "Epoch 9/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9766\n",
      "Epoch 00009: val_loss did not improve from 0.14592\n",
      "1108/1108 [==============================] - 177s 160ms/step - loss: 0.0981 - acc: 0.9766 - val_loss: 0.1786 - val_acc: 0.9614\n",
      "Epoch 10/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.0957 - acc: 0.9773\n",
      "Epoch 00010: val_loss improved from 0.14592 to 0.12403, saving model to /data/models/LabelDetectionModel.h5\n",
      "1108/1108 [==============================] - 177s 160ms/step - loss: 0.0957 - acc: 0.9773 - val_loss: 0.1240 - val_acc: 0.9693\n",
      "Epoch 11/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.0929 - acc: 0.9789\n",
      "Epoch 00011: val_loss did not improve from 0.12403\n",
      "1108/1108 [==============================] - 176s 159ms/step - loss: 0.0928 - acc: 0.9789 - val_loss: 0.1667 - val_acc: 0.9629\n",
      "Epoch 12/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9793\n",
      "Epoch 00012: val_loss did not improve from 0.12403\n",
      "1108/1108 [==============================] - 178s 161ms/step - loss: 0.0847 - acc: 0.9793 - val_loss: 0.1966 - val_acc: 0.9571\n",
      "Epoch 13/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9806\n",
      "Epoch 00013: val_loss did not improve from 0.12403\n",
      "1108/1108 [==============================] - 176s 159ms/step - loss: 0.0805 - acc: 0.9806 - val_loss: 0.2321 - val_acc: 0.9572\n",
      "Epoch 14/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9821\n",
      "Epoch 00014: val_loss did not improve from 0.12403\n",
      "1108/1108 [==============================] - 176s 159ms/step - loss: 0.0765 - acc: 0.9821 - val_loss: 0.1858 - val_acc: 0.9632\n",
      "Epoch 15/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9824\n",
      "Epoch 00015: val_loss did not improve from 0.12403\n",
      "1108/1108 [==============================] - 176s 159ms/step - loss: 0.0755 - acc: 0.9824 - val_loss: 0.1680 - val_acc: 0.9646\n",
      "Epoch 16/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9836\n",
      "Epoch 00016: val_loss did not improve from 0.12403\n",
      "1108/1108 [==============================] - 177s 159ms/step - loss: 0.0715 - acc: 0.9836 - val_loss: 0.1421 - val_acc: 0.9675\n",
      "Epoch 17/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9836\n",
      "Epoch 00017: val_loss did not improve from 0.12403\n",
      "1108/1108 [==============================] - 176s 159ms/step - loss: 0.0712 - acc: 0.9836 - val_loss: 0.2070 - val_acc: 0.9550\n",
      "Epoch 18/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9840\n",
      "Epoch 00018: val_loss did not improve from 0.12403\n",
      "1108/1108 [==============================] - 177s 160ms/step - loss: 0.0664 - acc: 0.9840 - val_loss: 0.2041 - val_acc: 0.9586\n",
      "Epoch 19/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9849\n",
      "Epoch 00019: val_loss did not improve from 0.12403\n",
      "1108/1108 [==============================] - 177s 160ms/step - loss: 0.0662 - acc: 0.9849 - val_loss: 0.1248 - val_acc: 0.9710\n",
      "Epoch 20/20\n",
      "1107/1108 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9848\n",
      "Epoch 00020: val_loss did not improve from 0.12403\n",
      "1108/1108 [==============================] - 176s 159ms/step - loss: 0.0644 - acc: 0.9847 - val_loss: 0.1875 - val_acc: 0.9644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.keras.callbacks.History at 0x7f8a4404ec50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "model.fit_generator(\n",
    "    generator.flow(X_train, y_train, batch_size=batch_size), \n",
    "    steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "    epochs=n_epoch,\n",
    "    validation_data=val_generator.flow(X_test, y_test, batch_size=batch_size),\n",
    "    validation_steps=X_test.shape[0] // batch_size,\n",
    "    callbacks=[\n",
    "        callbacks.LearningRateScheduler(lr_sched),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            MODEL_PATH,\n",
    "            monitor='val_loss',\n",
    "            verbose=1,\n",
    "            save_best_only=True),\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "[[139   0  18]\n",
      " [  6 388  20]\n",
      " [  0   0 429]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y = []\n",
    "Y_pred = []\n",
    "\n",
    "val_data = val_generator.flow(X_test, y_test, batch_size=1)\n",
    "\n",
    "for i in range(1000):\n",
    "    if i % 100 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "    lst, y_true = val_data.next()\n",
    "    y_true = np.argmax(y_true, axis=-1)\n",
    "    y_pred = np.argmax(model.predict(lst), axis=-1)\n",
    "    Y.append(y_true)\n",
    "    Y_pred.append(y_pred)\n",
    "    \n",
    "Y = np.concatenate(Y, axis=0)\n",
    "Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "\n",
    "print(\"\")\n",
    "cm = confusion_matrix(Y, Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
