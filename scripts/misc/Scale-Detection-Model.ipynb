{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Detection\n",
    "Train a model to detect the scale of an image relative to the scale of the training dataset for a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/deepcell/utils/__init__.py:49: UserWarning: To use `compute_overlap`, the C extensions must be built using `python setup.py build_ext --inplace`\n",
      "  warnings.warn('To use `compute_overlap`, the C extensions must be built '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import errno\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import deepcell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate all label dataset\n",
    "Download HeLa data for nuclear, brightfield and fluorescent cytoplasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.datasets import hela_s3, phase, cytoplasm\n",
    "\n",
    "nuc_train, nuc_test = hela_s3.load_data()\n",
    "brtfld_train, brtfld_test = phase.HeLa_S3.load_data()\n",
    "flr_train, flr_test = cytoplasm.hela_s3.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten All Datasets into 2D and Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped feature data from (6480, 216, 256, 1) to (6480, 216, 216, 1)\n",
      "Reshaped training data from (6480, 216, 256, 1) to (6480, 216, 216, 1)\n",
      "Reshaped feature data from (720, 216, 256, 1) to (720, 216, 216, 1)\n",
      "Reshaped training data from (720, 216, 256, 1) to (720, 216, 216, 1)\n",
      "Reshaped feature data from (576, 512, 512, 1) to (5184, 216, 216, 1)\n",
      "Reshaped training data from (576, 512, 512, 1) to (5184, 216, 216, 1)\n",
      "Reshaped feature data from (64, 512, 512, 1) to (576, 216, 216, 1)\n",
      "Reshaped training data from (64, 512, 512, 1) to (576, 216, 216, 1)\n",
      "Reshaped feature data from (576, 512, 512, 1) to (5184, 216, 216, 1)\n",
      "Reshaped training data from (576, 512, 512, 1) to (5184, 216, 216, 1)\n",
      "Reshaped feature data from (64, 512, 512, 1) to (576, 216, 216, 1)\n",
      "Reshaped training data from (64, 512, 512, 1) to (576, 216, 216, 1)\n"
     ]
    }
   ],
   "source": [
    "from deepcell.utils.data_utils import reshape_matrix\n",
    "\n",
    "RESHAPE_SIZE = 216\n",
    "\n",
    "for train, test in zip([nuc_train, brtfld_train, flr_train], \n",
    "                       [nuc_test, brtfld_test, flr_test]):\n",
    "    train['X'], train['y'] = reshape_matrix(train['X'], train['y'], RESHAPE_SIZE)\n",
    "    test['X'], test['y'] = reshape_matrix(test['X'], test['y'], RESHAPE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack up our data as train and test\n",
    "X_train = np.vstack([nuc_train['X'], brtfld_train['X'], flr_train['X']])\n",
    "y_train = np.vstack([nuc_train['y'], brtfld_train['y'], flr_train['y']])\n",
    "\n",
    "X_test = np.vstack([nuc_test['X'], brtfld_test['X'], flr_test['X']])\n",
    "y_test = np.vstack([nuc_test['y'], brtfld_test['y'], flr_test['y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a tensors as X and y\n",
    "X = np.vstack([X_train, X_test])\n",
    "y = np.vstack([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up filepath constants\n",
    "\n",
    "# change DATA_DIR if you are not using `deepcell.datasets`\n",
    "DATA_DIR = os.getcwd() #os.path.expanduser(os.path.join('~', '.keras', 'datasets'))\n",
    "\n",
    "# filename to write combined data\n",
    "filename = 'HeLa-mixed-labels.npz'\n",
    "\n",
    "# DATA_FILE should be a npz file, preferably from `make_training_data`\n",
    "DATA_FILE = os.path.join(DATA_DIR, filename)\n",
    "\n",
    "# the path to the data file is currently required for `train_model_()` functions\n",
    "np.savez(DATA_FILE, X=X, y=y)\n",
    "\n",
    "# confirm the data file is available\n",
    "assert os.path.isfile(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.image import Iterator\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.python.keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ScaleIterator(Iterator):\n",
    "    \"\"\"Iterator yielding data from Numpy arrayss (`X and `y`).\n",
    "\n",
    "    Args:\n",
    "        train_dict: dictionary consisting of numpy arrays for `X` and `y`.\n",
    "        image_data_generator: Instance of `ImageDataGenerator`\n",
    "            to use for random transformations and normalization.\n",
    "        batch_size: Integer, size of a batch.\n",
    "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
    "        seed: Random seed for data shuffling.\n",
    "        data_format: String, one of `channels_first`, `channels_last`.\n",
    "        save_to_dir: Optional directory where to save the pictures\n",
    "            being yielded, in a viewable format. This is useful\n",
    "            for visualizing the random transformations being\n",
    "            applied, for debugging purposes.\n",
    "        save_prefix: String prefix to use for saving sample\n",
    "            images (if `save_to_dir` is set).\n",
    "        save_format: Format to use for saving sample images\n",
    "            (if `save_to_dir` is set).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 train_dict,\n",
    "                 scale_generator,\n",
    "                 batch_size=1,\n",
    "                 skip=None,\n",
    "                 shuffle=False,\n",
    "                 transform=None,\n",
    "                 transform_kwargs={},\n",
    "                 seed=None,\n",
    "                 data_format='channels_last',\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='png'):\n",
    "        X, y = train_dict['X'], train_dict['y']\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError('Training batches and labels should have the same'\n",
    "                             'length. Found X.shape: {} y.shape: {}'.format(\n",
    "                                 X.shape, y.shape))\n",
    "        self.x = np.asarray(X, dtype=K.floatx())\n",
    "\n",
    "        if self.x.ndim != 4:\n",
    "            raise ValueError('Input data in `ImageFullyConvIterator` '\n",
    "                             'should have rank 4. You passed an array '\n",
    "                             'with shape', self.x.shape)\n",
    "\n",
    "        self.y = np.ones((self.x.shape[0],1), dtype=K.floatx())\n",
    "        self.channel_axis = 3 if data_format == 'channels_last' else 1\n",
    "        self.skip = skip\n",
    "        self.scale_generator = scale_generator\n",
    "        self.data_format = data_format\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "        super(ScaleIterator, self).__init__(\n",
    "            self.x.shape[0], batch_size, shuffle, seed)\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        batch_x = np.zeros(tuple([len(index_array)] + list(self.x.shape)[1:]))\n",
    "        batch_y = np.zeros(tuple([len(index_array)] + list(self.y.shape)[1:]))\n",
    "\n",
    "        for i, j in enumerate(index_array):\n",
    "            x = self.x[j]\n",
    "            x, y = self.scale_generator.random_transform(x.astype(K.floatx()))\n",
    "\n",
    "            x = self.scale_generator.standardize(x)\n",
    "\n",
    "            batch_x[i] = x\n",
    "            batch_y[i] = y\n",
    "\n",
    "        if self.save_to_dir:\n",
    "            for i, j in enumerate(index_array):\n",
    "                if self.data_format == 'channels_first':\n",
    "                    img_x = np.expand_dims(batch_x[i, 0, ...], 0)\n",
    "                else:\n",
    "                    img_x = np.expand_dims(batch_x[i, ..., 0], -1)\n",
    "                img = array_to_img(img_x, self.data_format, scale=True)\n",
    "                fname = '{prefix}_{index}_{hash}.{format}'.format(\n",
    "                    prefix=self.save_prefix,\n",
    "                    index=j,\n",
    "                    hash=np.random.randint(1e4),\n",
    "                    format=self.save_format)\n",
    "                img.save(os.path.join(self.save_to_dir, fname))\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x. Returns the next batch.\n",
    "        \"\"\"\n",
    "        # Keeps under lock only the mechanism which advances\n",
    "        # the indexing of each batch.\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        return self._get_batches_of_transformed_samples(index_array)\n",
    "\n",
    "class ScaleDataGenerator(ImageDataGenerator):\n",
    "    \"\"\"Generates batches of tensor image data with real-time data augmentation.\n",
    "    The data will be looped over (in batches).\n",
    "\n",
    "    Args:\n",
    "        featurewise_center: boolean, set input mean to 0 over the dataset,\n",
    "            feature-wise.\n",
    "        samplewise_center: boolean, set each sample mean to 0.\n",
    "        featurewise_std_normalization: boolean, divide inputs by std\n",
    "            of the dataset, feature-wise.\n",
    "        samplewise_std_normalization: boolean, divide each input by its std.\n",
    "        zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
    "        zca_whitening: boolean, apply ZCA whitening.\n",
    "        rotation_range: int, degree range for random rotations.\n",
    "        width_shift_range: float, 1-D array-like or int\n",
    "            float: fraction of total width, if < 1, or pixels if >= 1.\n",
    "            1-D array-like: random elements from the array.\n",
    "            int: integer number of pixels from interval\n",
    "                `(-width_shift_range, +width_shift_range)`\n",
    "            With `width_shift_range=2` possible values are ints [-1, 0, +1],\n",
    "            same as with `width_shift_range=[-1, 0, +1]`,\n",
    "            while with `width_shift_range=1.0` possible values are floats in\n",
    "            the interval [-1.0, +1.0).\n",
    "        shear_range: float, shear Intensity\n",
    "            (Shear angle in counter-clockwise direction in degrees)\n",
    "        zoom_range: float or [lower, upper], Range for random zoom.\n",
    "            If a float, `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n",
    "        channel_shift_range: float, range for random channel shifts.\n",
    "        fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.\n",
    "            Default is 'nearest'. Points outside the boundaries of the input\n",
    "            are filled according to the given mode:\n",
    "                'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
    "                'nearest':  aaaaaaaa|abcd|dddddddd\n",
    "                'reflect':  abcddcba|abcd|dcbaabcd\n",
    "                'wrap':  abcdabcd|abcd|abcdabcd\n",
    "        cval: float or int, value used for points outside the boundaries\n",
    "            when `fill_mode = \"constant\"`.\n",
    "        horizontal_flip: boolean, randomly flip inputs horizontally.\n",
    "        vertical_flip: boolean, randomly flip inputs vertically.\n",
    "        rescale: rescaling factor. Defaults to None. If None or 0, no rescaling\n",
    "            is applied, otherwise we multiply the data by the value provided\n",
    "            (before applying any other transformation).\n",
    "        preprocessing_function: function that will be implied on each input.\n",
    "            The function will run after the image is resized and augmented.\n",
    "            The function should take one argument:\n",
    "            one image (Numpy tensor with rank 3),\n",
    "            and should output a Numpy tensor with the same shape.\n",
    "        data_format: One of {\"channels_first\", \"channels_last\"}.\n",
    "            \"channels_last\" mode means that the images should have shape\n",
    "                `(samples, height, width, channels)`,\n",
    "            \"channels_first\" mode means that the images should have shape\n",
    "                `(samples, channels, height, width)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "                Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"channels_last\".\n",
    "        validation_split: float, fraction of images reserved for validation\n",
    "            (strictly between 0 and 1).\n",
    "    \"\"\"\n",
    "\n",
    "    def flow(self,\n",
    "             train_dict,\n",
    "             batch_size=1,\n",
    "             skip=None,\n",
    "             transform=None,\n",
    "             transform_kwargs={},\n",
    "             shuffle=True,\n",
    "             seed=None,\n",
    "             save_to_dir=None,\n",
    "             save_prefix='',\n",
    "             save_format='png'):\n",
    "        \"\"\"Generates batches of augmented/normalized data with given arrays.\n",
    "\n",
    "        Args:\n",
    "            train_dict: dictionary of X and y tensors. Both should be rank 4.\n",
    "            batch_size: int (default: 1).\n",
    "            shuffle: boolean (default: True).\n",
    "            seed: int (default: None).\n",
    "            save_to_dir: None or str (default: None).\n",
    "                This allows you to optionally specify a directory\n",
    "                to which to save the augmented pictures being generated\n",
    "                (useful for visualizing what you are doing).\n",
    "            save_prefix: str (default: `''`). Prefix to use for filenames of\n",
    "                saved pictures (only relevant if `save_to_dir` is set).\n",
    "            save_format: one of \"png\", \"jpeg\". Default: \"png\".\n",
    "                (only relevant if `save_to_dir` is set)\n",
    "\n",
    "        Returns:\n",
    "            An Iterator yielding tuples of `(x, y)` where `x` is a numpy array\n",
    "            of image data and `y` is a numpy array of labels of the same shape.\n",
    "        \"\"\"\n",
    "        return ScaleIterator(\n",
    "            train_dict,\n",
    "            self,\n",
    "            batch_size=batch_size,\n",
    "            transform=transform,\n",
    "            transform_kwargs=transform_kwargs,\n",
    "            skip=skip,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            data_format=self.data_format,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format)\n",
    "\n",
    "    def random_transform(self, x, y=None, seed=None):\n",
    "        \"\"\"Applies a random transformation to an image.\n",
    "\n",
    "        Args:\n",
    "            x: 3D tensor or list of 3D tensors,\n",
    "                single image.\n",
    "            y: 3D tensor or list of 3D tensors,\n",
    "                label mask(s) for `x`, optional.\n",
    "            seed: Random seed.\n",
    "\n",
    "        Returns:\n",
    "            A randomly transformed version of the input (same shape).\n",
    "            If `y` is passed, it is transformed if necessary and returned.\n",
    "        \"\"\"\n",
    "        params = self.get_random_transform(x.shape, seed)\n",
    "        params['zy'] = params['zx']\n",
    "\n",
    "        if isinstance(x, list):\n",
    "            x = [self.apply_transform(x_i, params) for x_i in x]\n",
    "        else:\n",
    "            x = self.apply_transform(x, params)\n",
    "        \n",
    "        y = np.ones(1)\n",
    "        y[0] = params['zx']\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Input, AveragePooling2D, Flatten, Activation\n",
    "from deepcell.layers import ImageNormalization2D, TensorProduct\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from deepcell.utils.backbone_utils import get_backbone\n",
    "\n",
    "def scale_model(input_shape=(None, None, 1),\n",
    "                inputs=None,\n",
    "                backbone='VGG16',\n",
    "                use_imagenet=True,\n",
    "                required_channels=3,\n",
    "                norm_method='whole_image',\n",
    "                pooling=None):\n",
    "    if inputs is None:\n",
    "        inputs = Input(shape=input_shape)\n",
    "    \n",
    "    norm = ImageNormalization2D(norm_method=norm_method)(inputs)\n",
    "    fixed_inputs = TensorProduct(required_channels)(norm)\n",
    "    \n",
    "    # force the input shape\n",
    "    fixed_input_shape = list(input_shape)\n",
    "    fixed_input_shape[-1] = required_channels\n",
    "    fixed_input_shape = tuple(fixed_input_shape)\n",
    "\n",
    "    model_kwargs = {\n",
    "        'include_top': False,\n",
    "        'weights': None,\n",
    "        'input_shape': fixed_input_shape,\n",
    "        'pooling': pooling\n",
    "    }\n",
    "    \n",
    "    backbone = get_backbone(backbone, fixed_inputs, use_imagenet=use_imagenet, return_dict=False, **model_kwargs)\n",
    "    \n",
    "    backbone_inputs = backbone.inputs\n",
    "    backbone_outputs = backbone.outputs[0]\n",
    "        \n",
    "    new_model_inputs = backbone_inputs\n",
    "    x = AveragePooling2D(4)(backbone_outputs)\n",
    "    x = TensorProduct(256)(x)\n",
    "    x = TensorProduct(1)(x)\n",
    "    x = Flatten()(x)\n",
    "    new_model_outputs = Activation('relu')(x)\n",
    "    \n",
    "    new_model = Model(inputs=new_model_inputs, outputs=new_model_outputs)\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 216, 216, 1)       0         \n",
      "_________________________________________________________________\n",
      "image_normalization2d (Image (None, 216, 216, 1)       3721      \n",
      "_________________________________________________________________\n",
      "tensor_product (TensorProduc (None, 216, 216, 3)       6         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 216, 216, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 216, 216, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 108, 108, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 108, 108, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 108, 108, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 54, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 54, 54, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 54, 54, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 54, 54, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 27, 27, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 27, 27, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 27, 27, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "tensor_product_1 (TensorProd (None, 1, 1, 256)         131328    \n",
      "_________________________________________________________________\n",
      "tensor_product_2 (TensorProd (None, 1, 1, 1)           257       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 14,850,000\n",
      "Trainable params: 14,846,279\n",
      "Non-trainable params: 3,721\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trial_model = scale_model(input_shape=(216,216,1))\n",
    "trial_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "n_epoch = 20  # Number of training epochs\n",
    "lr = 1e-5\n",
    "optimizer = Adam(lr=lr, clipnorm=0.001)\n",
    "lr_sched = rate_scheduler(lr=lr, decay=0.99)\n",
    "batch_size = 4\n",
    "\n",
    "trial_model.compile(optimizer, loss='MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/notebooks/data/ScaleDetection-helaMultiLabelv3.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "train_dict = {}\n",
    "train_dict['X'] = X_train\n",
    "train_dict['y'] = y_train\n",
    "\n",
    "gen = ScaleDataGenerator(rotation_range=180,\n",
    "                        horizontal_flip=True,\n",
    "                        vertical_flip=True,\n",
    "                        zoom_range=(0.5, 2))\n",
    "\n",
    "train_data = gen.flow(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4212/4212 [==============================] - 137s 32ms/step - loss: 0.0296\n",
      "Epoch 2/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 0.0036\n",
      "Epoch 3/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 0.0024\n",
      "Epoch 4/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 0.0019\n",
      "Epoch 5/20\n",
      "4211/4212 [============================>.] - ETA: 0s - loss: 0.0018\n",
      "Epoch 00005: saving model to /notebooks/data/ScaleDetection-helaMultiLabelv3.h5\n",
      "4212/4212 [==============================] - 133s 31ms/step - loss: 0.0018\n",
      "Epoch 6/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 0.0015\n",
      "Epoch 7/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 0.0014\n",
      "Epoch 8/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 0.0012\n",
      "Epoch 9/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 0.0010\n",
      "Epoch 10/20\n",
      "4211/4212 [============================>.] - ETA: 0s - loss: 9.6497e-04\n",
      "Epoch 00010: saving model to /notebooks/data/ScaleDetection-helaMultiLabelv3.h5\n",
      "4212/4212 [==============================] - 133s 31ms/step - loss: 9.6475e-04\n",
      "Epoch 11/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 9.0407e-04\n",
      "Epoch 12/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 7.3361e-04\n",
      "Epoch 13/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 8.9422e-04\n",
      "Epoch 14/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 7.4633e-04\n",
      "Epoch 15/20\n",
      "4211/4212 [============================>.] - ETA: 0s - loss: 6.7739e-04\n",
      "Epoch 00015: saving model to /notebooks/data/ScaleDetection-helaMultiLabelv3.h5\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 6.7726e-04\n",
      "Epoch 16/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 6.7605e-04\n",
      "Epoch 17/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 6.5743e-04\n",
      "Epoch 18/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 5.8436e-04\n",
      "Epoch 19/20\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 6.0024e-04\n",
      "Epoch 20/20\n",
      "4211/4212 [============================>.] - ETA: 0s - loss: 5.4129e-04\n",
      "Epoch 00020: saving model to /notebooks/data/ScaleDetection-helaMultiLabelv3.h5\n",
      "4212/4212 [==============================] - 132s 31ms/step - loss: 5.4621e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2ee008b0f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras import callbacks\n",
    "\n",
    "trial_model.fit_generator(train_data, \n",
    "                          steps_per_epoch=train_data.y.shape[0]//batch_size,\n",
    "                          epochs=n_epoch,\n",
    "                          callbacks=[\n",
    "                            callbacks.LearningRateScheduler(lr_sched),\n",
    "                            callbacks.ModelCheckpoint(\n",
    "                                model_path, verbose=1, period=5)\n",
    "                        ])\n",
    "trial_model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen3 = ScaleDataGenerator(rotation_range=180,\n",
    "                        horizontal_flip=True,\n",
    "                        vertical_flip=True,\n",
    "                        zoom_range=(0.5, 2))\n",
    "\n",
    "test_data = gen3.flow(test_dict)\n",
    "\n",
    "true,pred = [],[]\n",
    "for i in range(500):\n",
    "    X,y = test_data.next()\n",
    "    true.append(y)\n",
    "    pred.append(trial_model.predict(X))\n",
    "    \n",
    "true = np.array(true)\n",
    "pred = np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'True Zoom')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(true,pred-true)\n",
    "ax.set_xlabel('True Zoom')\n",
    "ax.set_ylabel('Error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
