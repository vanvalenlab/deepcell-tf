{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model on an Entire Image\n",
    "\n",
    "The images we have trained on have been cut into smaller sizes for annotation purposes.  Now that we have a trained model, we can run it on the entire raw image.  Because Z-stacks are quite large, we use a wrapper function `process_whole_image` to slice the raw image into several crops, and stitch the predictions back into place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the raw images from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.utils.data_utils import load_training_images_3d\n",
    "\n",
    "whole_images = load_training_images_3d(\n",
    "    '/data/data/cells/MouseBrain/generic',\n",
    "    training_direcs=['set0', 'set1'],\n",
    "    num_frames=30,\n",
    "    raw_image_direc='raw',\n",
    "    channel_names=['DAPI'],\n",
    "    image_size=(1024, 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the input shape\n",
    "\n",
    "The whole images must be padded so the model output will be the same size as the input.\n",
    "\n",
    "Use `get_cropped_input_shape` to instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.running import get_cropped_input_shape\n",
    "\n",
    "# get the size of each cropped Z-stack\n",
    "cropped_input = get_cropped_input_shape(\n",
    "    whole_images, num_crops=4, receptive_field=61)\n",
    "\n",
    "print('Whole Image shape:', whole_images.shape[1:])\n",
    "print('Cropped Input Shape:', cropped_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the full-sized model\n",
    "\n",
    "Re-create the model with the same parameters used during training except with the new `cropped_input_shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training parameters\n",
    "frames_per_batch = 3\n",
    "n_skips = 3\n",
    "model_dir = os.path.join(os.getcwd(), 'models')\n",
    "\n",
    "# Re-instantiate the foreground/background model\n",
    "fgbg_model = bn_feature_net_skip_3D(\n",
    "    receptive_field=61,\n",
    "    n_skips=n_skips,\n",
    "    n_features=2,  # segmentation mask (is_cell, is_not_cell)\n",
    "    n_frames=frames_per_batch,\n",
    "    input_shape=cropped_input,\n",
    "    n_conv_filters=32,\n",
    "    n_dense_filters=128,\n",
    "    last_only=False)\n",
    "\n",
    "# Load the FGBG weights\n",
    "fgbg_weights_file = '2018-09-15_MouseBrain_3d_nuclear_fgbg.h5'  # use custom file\n",
    "fgbg_weights_file = os.path.join(model_dir, fgbg_weights_file)\n",
    "fgbg_model.load_weights(fgbg_weights_file)\n",
    "\n",
    "\n",
    "# Re-instatiate the conv model\n",
    "run_conv_model = bn_feature_net_skip_3D(\n",
    "    fgbg_model=fgbg_model,\n",
    "    n_features=4,  # number of output classes\n",
    "    n_skips=n_skips,\n",
    "    n_frames=frames_per_batch,\n",
    "    input_shape=cropped_input,\n",
    "    n_conv_filters=32,\n",
    "    n_dense_filters=128,\n",
    "    last_only=True)\n",
    "\n",
    "# Load the conv weights\n",
    "conv_weights_file = '2018-09-15_MouseBrain_3d_nuclear_conv.h5'  # use custom file\n",
    "conv_weights_file = os.path.join(model_dir, conv_weights_file)\n",
    "run_conv_model.load_weights(conv_weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the entire image\n",
    "\n",
    "Use the built-in function `process_whole_image` to iteratively predict and stitch together each of the slices of the large image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.running import process_whole_image\n",
    "\n",
    "output = process_whole_image(\n",
    "    model=run_conv_model,\n",
    "    images=whole_images,\n",
    "    num_crops=4,\n",
    "    receptive_field=61)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = 1\n",
    "frame = 10\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(15, 15), sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(whole_images[index, frame, ..., 0], cmap='gray')\n",
    "ax[0].set_title('Source Image')\n",
    "\n",
    "ax[1].imshow(output[index, frame, ..., 0] + output[index, frame, ..., 1], cmap='jet')\n",
    "ax[1].set_title('Edge Segmentation Prediction')\n",
    "\n",
    "ax[2].imshow(output[index, frame, ..., 2], cmap='jet')\n",
    "ax[2].set_title('Interior Segmentation Prediction')\n",
    "\n",
    "ax[3].imshow(np.argmax(output[index, frame, ...], axis=-1), cmap='jet')\n",
    "ax[3].set_title('Argmax Prediction')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.utils.plot_utils import get_js_video\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(get_js_video(output, batch=0, channel=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
