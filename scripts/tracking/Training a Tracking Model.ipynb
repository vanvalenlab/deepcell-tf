{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell Tracking and Lineage Construction in Live-Cell Imaging Data\n",
    "\n",
    "### Part 1 (of 2)\n",
    "\n",
    "## Training a Model\n",
    "---\n",
    "\n",
    "Implementation of:\n",
    "\n",
    "[Accurate cell tracking and lineage construction in live-cell imaging experiments with deep learning](https://www.biorxiv.org/content/10.1101/803205v2)\n",
    "\n",
    "Deployed at:\n",
    "\n",
    "[DeepCell.org](http://www.deepcell.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import errno\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import deepcell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3T3 -\n",
      "X.shape: (192, 30, 154, 182, 1)\n",
      "y.shape: (192, 30, 154, 182, 1)\n",
      "HeLa -\n",
      "X.shape: (144, 40, 216, 256, 1)\n",
      "y.shape: (144, 40, 216, 256, 1)\n",
      "HEK293 -\n",
      "X.shape: (207, 30, 135, 160, 1)\n",
      "y.shape: (207, 30, 135, 160, 1)\n",
      "RAW264.7 -\n",
      "X.shape: (99, 30, 202, 240, 1)\n",
      "y.shape: (99, 30, 202, 240, 1)\n"
     ]
    }
   ],
   "source": [
    "# Training a tracking algorithm is a complicated process that requires alot of data\n",
    "# We recommend combining multiple data sets  \n",
    "\n",
    "# Download four different sets of data (saves to ~/.keras/datasets)\n",
    "filename_3T3 = '3T3_NIH.trks'\n",
    "(X_train, y_train), (X_test, y_test) = deepcell.datasets.tracked.nih_3t3.load_tracked_data(filename_3T3)\n",
    "print('3T3 -\\nX.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))\n",
    "\n",
    "filename_HeLa = 'HeLa_S3.trks'\n",
    "(X_train, y_train), (X_test, y_test) = deepcell.datasets.tracked.hela_s3.load_tracked_data(filename_HeLa)\n",
    "print('HeLa -\\nX.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))\n",
    "\n",
    "filename_HEK = 'HEK293.trks'\n",
    "(X_train, y_train), (X_test, y_test) = deepcell.datasets.tracked.hek293.load_tracked_data(filename_HEK)\n",
    "print('HEK293 -\\nX.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))\n",
    "\n",
    "filename_RAW = 'RAW2647.trks'\n",
    "(X_train, y_train), (X_test, y_test) = deepcell.datasets.tracked.raw2647.load_tracked_data(filename_RAW)\n",
    "print('RAW264.7 -\\nX.shape: {}\\ny.shape: {}'.format(X_train.shape, y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: \n",
      "Image data shape:  (803, 40, 216, 256, 1)\n",
      "Number of lineages (should equal batch size):  803\n",
      "Total number of unique tracks (cells)      -  12697\n",
      "Total number of divisions                  -  944\n",
      "Average cell density (cells/100 sq pixels) -  0.017033540852301552\n",
      "Average number of frames per track         -  25\n"
     ]
    }
   ],
   "source": [
    "# Compile multiple TRKS files into one\n",
    "\n",
    "# NB: .trks files are a special format that includes image and lineage data in np arrays\n",
    "# To access, use the following utils\n",
    "from deepcell.utils.tracking_utils import load_trks\n",
    "from deepcell.utils.tracking_utils import save_trks\n",
    "\n",
    "# Define a normalizaiton function for the raw images that can be run before padding\n",
    "def image_norm(original_image):\n",
    "    # NNs prefer input data that is 0 mean and unit variance\n",
    "    normed_image = (original_image - np.mean(original_image)) / np.std(original_image)\n",
    "    return normed_image\n",
    "\n",
    "# Define all the trks to load\n",
    "basepath = os.path.expanduser(os.path.join('~', '.keras', 'datasets'))\n",
    "trks_files = [os.path.join(basepath, filename_3T3), \n",
    "              os.path.join(basepath, filename_HeLa), \n",
    "              os.path.join(basepath, filename_HEK),\n",
    "              os.path.join(basepath, filename_RAW)]\n",
    "\n",
    "# Each TRKS file may have differrent dimensions but the model expects uniform dimensions\n",
    "# So we will determine max dimensions and zero pad as neccesary\n",
    "max_frames = 1\n",
    "max_y = 1\n",
    "max_x = 1\n",
    "\n",
    "for trks_file in trks_files:\n",
    "    trks = load_trks(trks_file)\n",
    "\n",
    "    # Store dimensions of raw and tracked to check new data against to pad if neccesary\n",
    "    if trks['X'][0].shape[0] > max_frames:\n",
    "        max_frames = trks['X'][0].shape[0]\n",
    "    if trks['X'][0].shape[1] > max_y:\n",
    "        max_y = trks['X'][0].shape[1]\n",
    "    if trks['X'][0].shape[2] > max_x:\n",
    "        max_x = trks['X'][0].shape[2]\n",
    "\n",
    "# Load each trks file, normalize and pad as neccesary\n",
    "lineages = []\n",
    "X = []\n",
    "y = []        \n",
    "\n",
    "k = 0\n",
    "movie_counter = 0\n",
    "for trks_file in trks_files:\n",
    "    trks = load_trks(trks_file)\n",
    "    for i, (lineage, raw, tracked) in enumerate(zip(trks['lineages'], trks['X'], trks['y'])):\n",
    "        movie_counter = k + i\n",
    "\n",
    "        # Normalize the raw images\n",
    "        for frame in range(raw.shape[0]):\n",
    "            raw[frame, :, :, 0] = image_norm(raw[frame, :, :, 0]) \n",
    "            \n",
    "        # Image padding if neccesary - This assumes that raw and tracked have the same shape\n",
    "        if raw.shape[1] < max_y:\n",
    "            diff2pad = max_y-raw.shape[1]\n",
    "            pad_width = int(diff2pad/2)\n",
    "            if diff2pad % 2 == 0:\n",
    "                # Pad width can be split evenly\n",
    "                raw = np.pad(raw, ((0,0), (pad_width,pad_width), (0,0), (0,0)), mode='constant', constant_values=0)\n",
    "                tracked = np.pad(tracked, ((0,0), (pad_width,pad_width), (0,0), (0,0)), mode='constant', constant_values=0)\n",
    "            else:\n",
    "                # Pad width cannot be split evenly\n",
    "                raw = np.pad(raw, ((0,0), (pad_width+1,pad_width), (0,0), (0,0)), mode='constant', constant_values=0)\n",
    "                tracked = np.pad(tracked, ((0,0), (pad_width+1,pad_width), (0,0), (0,0)), mode='constant', constant_values=0)\n",
    "\n",
    "        if raw.shape[2] < max_x:\n",
    "            diff2pad = max_x-raw.shape[2]\n",
    "            pad_width = int(diff2pad/2)\n",
    "            if diff2pad % 2 == 0:\n",
    "                # Pad width can be split evenly\n",
    "                raw = np.pad(raw, ((0,0), (0,0), (pad_width,pad_width), (0,0)), mode='constant', constant_values=0)\n",
    "                tracked = np.pad(tracked, ((0,0), (0,0), (pad_width,pad_width), (0,0)), mode='constant', constant_values=0)\n",
    "            else:\n",
    "                # Pad width cannot be split evenly\n",
    "                raw = np.pad(raw, ((0,0), (0,0), (pad_width+1,pad_width), (0,0)), mode='constant', constant_values=0)\n",
    "                tracked = np.pad(tracked, ((0,0), (0,0), (pad_width+1,pad_width), (0,0)), mode='constant', constant_values=0)\n",
    "        \n",
    "        if raw.shape[0] < max_frames:   \n",
    "            pad_width = int(max_frames-raw.shape[0])\n",
    "            raw = np.pad(raw, ((0,pad_width), (0,0), (0,0), (0,0)), mode='constant', constant_values=0)\n",
    "            tracked = np.pad(tracked, ((0,pad_width), (0,0), (0,0), (0,0)), mode='constant', constant_values=0)\n",
    "        \n",
    "        lineages.append(lineage)\n",
    "        X.append(raw)\n",
    "        y.append(tracked)\n",
    "                \n",
    "    k = movie_counter + 1\n",
    "\n",
    "# Save the combined datasets into one trks file\n",
    "filename = 'combined_data.trks'\n",
    "save_trks(os.path.join(basepath, filename), lineages, X, y)\n",
    "\n",
    "# View stats on this combined file\n",
    "from deepcell.utils.tracking_utils import trks_stats\n",
    "trks_stats(os.path.join(basepath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: \n",
      "Image data shape:  (722, 40, 216, 256, 1)\n",
      "Number of lineages (should equal batch size):  722\n",
      "Total number of unique tracks (cells)      -  11510\n",
      "Total number of divisions                  -  844\n",
      "Average cell density (cells/100 sq pixels) -  0.017189596498441827\n",
      "Average number of frames per track         -  25\n"
     ]
    }
   ],
   "source": [
    "# combined_data.trks contains all of the data available\n",
    "\n",
    "# To hold out a portion of this data for testing we will establish a random seed\n",
    "test_seed = 1\n",
    "\n",
    "# And how much of the data to hold out\n",
    "test_size = .1\n",
    "\n",
    "# Get the full dataset\n",
    "trks = load_trks(os.path.join(basepath, filename))\n",
    "total_data_size = trks['X'].shape[0]\n",
    "\n",
    "# Select a portion of this dataset randomly \n",
    "import random\n",
    "random.seed(test_seed)\n",
    "train_data_range = int(total_data_size*(1-test_size))\n",
    "\n",
    "idx_train = random.sample(range(total_data_size), train_data_range)\n",
    "\n",
    "lineages, X, y = [], [], []\n",
    "for i in idx_train:\n",
    "    lineages.append(trks['lineages'][i])\n",
    "    X.append(trks['X'][i])\n",
    "    y.append(trks['y'][i])       \n",
    "\n",
    "# Resave the portion we wish to use as the training (and validation) dataset\n",
    "filename_train = 'combined_training_data.trks'\n",
    "save_trks(os.path.join(basepath, filename_train), lineages, X, y)\n",
    "\n",
    "# View stats on this combined file\n",
    "trks_stats(os.path.join(basepath, filename_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up filepath constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the data file is currently required for `train_model_()` functions\n",
    "\n",
    "# Change DATA_DIR if you are not using `deepcell.datasets`\n",
    "DATA_DIR = os.path.expanduser(os.path.join('~', '.keras', 'datasets'))\n",
    "\n",
    "# DATA_FILE should be a trks file (contains 2 np arrays and a lineage dictionary)\n",
    "DATA_FILE = os.path.join(DATA_DIR, filename_train)\n",
    "\n",
    "# confirm the data file is available\n",
    "assert os.path.isfile(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up other required filepaths\n",
    "\n",
    "# If the data file is in a subdirectory, mirror it in MODEL_DIR and LOG_DIR\n",
    "PREFIX = os.path.relpath(os.path.dirname(DATA_FILE), DATA_DIR)\n",
    "\n",
    "ROOT_DIR = '/data'  # TODO: Change this! Usually a mounted volume\n",
    "MODEL_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'models', PREFIX))\n",
    "LOG_DIR = os.path.abspath(os.path.join(ROOT_DIR, 'logs', PREFIX))\n",
    "\n",
    "# create directories if they do not exist\n",
    "for d in (MODEL_DIR, LOG_DIR):\n",
    "    try:\n",
    "        os.makedirs(d)\n",
    "    except OSError as exc:  # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a New Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from deepcell.utils.train_utils import rate_scheduler\n",
    "\n",
    "n_epoch = 10     # Number of training epochs\n",
    "test_size = .20  # % of data saved as validation\n",
    "train_seed = 1   # Random seed for training/validation data split\n",
    "\n",
    "optimizer = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "lr_sched = rate_scheduler(lr=0.01, decay=0.99)\n",
    "\n",
    "# Tracking training settings\n",
    "features = {'appearance', 'distance', 'neighborhood', 'regionprop'}\n",
    "min_track_length = 9\n",
    "neighborhood_scale_size=30\n",
    "batch_size = 128  \n",
    "crop_dim = 32\n",
    "in_shape = (crop_dim, crop_dim, 1)\n",
    "\n",
    "tracking_model_name = 'tracking_model_seed{}_tl{}'.format(train_seed, min_track_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the tracking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell import model_zoo\n",
    "\n",
    "tracking_model = model_zoo.siamese_model(\n",
    "    input_shape=in_shape,\n",
    "    neighborhood_scale_size=neighborhood_scale_size,\n",
    "    features=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a new tracking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on dataset: /root/.keras/datasets/combined_training_data.trks\n",
      "saving model at: /data/models/tracking_model_1_tl9.h5\n",
      "saving loss at: /data/models/tracking_model_1_tl9.npz\n",
      "X_train shape: (577, 40, 216, 256, 1)\n",
      "y_train shape: (577, 40, 216, 256, 1)\n",
      "X_test shape: (145, 40, 216, 256, 1)\n",
      "y_test shape: (145, 40, 216, 256, 1)\n",
      "Output Shape: (None, 3)\n",
      "Training on 1 GPUs\n",
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/measure/_regionprops.py:250: UserWarning: regionprops and image moments (including moments, normalized moments, central moments, and inertia tensor) of 2D images will change from xy coordinates to rc coordinates in version 0.16.\n",
      "See https://scikit-image.org/docs/0.14.x/release_notes_and_installation.html#deprecations for details on how to avoid this message.\n",
      "  warn(XY_TO_RC_DEPRECATION_MESSAGE)\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/measure/_regionprops.py:260: UserWarning: regionprops and image moments (including moments, normalized moments, central moments, and inertia tensor) of 2D images will change from xy coordinates to rc coordinates in version 0.16.\n",
      "See https://scikit-image.org/docs/0.14.x/release_notes_and_installation.html#deprecations for details on how to avoid this message.\n",
      "  warn(XY_TO_RC_DEPRECATION_MESSAGE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_train_pairs: 708688.0\n",
      "total_test_pairs: 203512.0\n",
      "batch size: 128\n",
      "validation_steps:  1589.0\n",
      "Epoch 1/10\n",
      "5535/5536 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9826\n",
      "Epoch 00001: val_loss improved from inf to 0.09630, saving model to /data/models/tracking_model_1_tl9.h5\n",
      "5536/5536 [==============================] - 4391s 793ms/step - loss: 0.0607 - acc: 0.9826 - val_loss: 0.0963 - val_acc: 0.9781\n",
      "Epoch 2/10\n",
      "5535/5536 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9948\n",
      "Epoch 00002: val_loss did not improve from 0.09630\n",
      "5536/5536 [==============================] - 4375s 790ms/step - loss: 0.0282 - acc: 0.9948 - val_loss: 0.0967 - val_acc: 0.9785\n",
      "Epoch 3/10\n",
      " 284/5536 [>.............................] - ETA: 1:15:34 - loss: 0.0258 - acc: 0.9961WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.315415). Check your callbacks.\n",
      "2206/5536 [==========>...................] - ETA: 46:48 - loss: 0.0236 - acc: 0.9964WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.412171). Check your callbacks.\n",
      "2207/5536 [==========>...................] - ETA: 46:47 - loss: 0.0236 - acc: 0.9964WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.412525). Check your callbacks.\n",
      "3364/5536 [=================>............] - ETA: 30:07 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.724179). Check your callbacks.\n",
      "3365/5536 [=================>............] - ETA: 30:06 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.724179). Check your callbacks.\n",
      "3366/5536 [=================>............] - ETA: 30:06 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.724179). Check your callbacks.\n",
      "3367/5536 [=================>............] - ETA: 30:05 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.723621). Check your callbacks.\n",
      "3368/5536 [=================>............] - ETA: 30:05 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.723621). Check your callbacks.\n",
      "3369/5536 [=================>............] - ETA: 30:04 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.723621). Check your callbacks.\n",
      "3370/5536 [=================>............] - ETA: 30:05 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.724031). Check your callbacks.\n",
      "3371/5536 [=================>............] - ETA: 30:04 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.724445). Check your callbacks.\n",
      "3372/5536 [=================>............] - ETA: 30:04 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.724383). Check your callbacks.\n",
      "3373/5536 [=================>............] - ETA: 30:03 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.724383). Check your callbacks.\n",
      "3374/5536 [=================>............] - ETA: 30:03 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.724383). Check your callbacks.\n",
      "3375/5536 [=================>............] - ETA: 30:02 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.724383). Check your callbacks.\n",
      "3376/5536 [=================>............] - ETA: 30:02 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.724383). Check your callbacks.\n",
      "3377/5536 [=================>............] - ETA: 30:01 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.724383). Check your callbacks.\n",
      "3397/5536 [=================>............] - ETA: 29:49 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.410503). Check your callbacks.\n",
      "3398/5536 [=================>............] - ETA: 29:48 - loss: 0.0234 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.410503). Check your callbacks.\n",
      "5065/5536 [==========================>...] - ETA: 6:32 - loss: 0.0232 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.319481). Check your callbacks.\n",
      "5066/5536 [==========================>...] - ETA: 6:31 - loss: 0.0232 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.319431). Check your callbacks.\n",
      "5067/5536 [==========================>...] - ETA: 6:31 - loss: 0.0232 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.319431). Check your callbacks.\n",
      "5068/5536 [==========================>...] - ETA: 6:30 - loss: 0.0232 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.319431). Check your callbacks.\n",
      "5069/5536 [==========================>...] - ETA: 6:29 - loss: 0.0232 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.319431). Check your callbacks.\n",
      "5070/5536 [==========================>...] - ETA: 6:28 - loss: 0.0232 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.319431). Check your callbacks.\n",
      "5071/5536 [==========================>...] - ETA: 6:28 - loss: 0.0232 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.319431). Check your callbacks.\n",
      "5072/5536 [==========================>...] - ETA: 6:27 - loss: 0.0232 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.319306). Check your callbacks.\n",
      "5073/5536 [==========================>...] - ETA: 6:26 - loss: 0.0232 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.309752). Check your callbacks.\n",
      "5074/5536 [==========================>...] - ETA: 6:25 - loss: 0.0232 - acc: 0.9965WARNING:tensorflow:Method on_batch_end() is slow compared to the batch update (0.310338). Check your callbacks.\n",
      "5535/5536 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9965\n",
      "Epoch 00003: val_loss did not improve from 0.09630\n",
      "5536/5536 [==============================] - 4799s 867ms/step - loss: 0.0231 - acc: 0.9965 - val_loss: 0.1174 - val_acc: 0.9760\n",
      "Epoch 4/10\n",
      "3385/5536 [=================>............] - ETA: 27:29 - loss: 0.0211 - acc: 0.9972"
     ]
    }
   ],
   "source": [
    "from deepcell.training import train_model_siamese_daughter\n",
    "\n",
    "tracking_model = train_model_siamese_daughter(\n",
    "    model=tracking_model,\n",
    "    dataset=DATA_FILE,  # full path to trks file\n",
    "    model_name=tracking_model_name,\n",
    "    optimizer=optimizer,\n",
    "    batch_size=batch_size,\n",
    "    crop_dim=crop_dim,\n",
    "    min_track_length=min_track_length,\n",
    "    features=features,\n",
    "    neighborhood_scale_size=neighborhood_scale_size,\n",
    "    test_size=test_size,\n",
    "    n_epoch=n_epoch,\n",
    "    model_dir=MODEL_DIR,\n",
    "    lr_sched=lr_sched,\n",
    "    rotation_range=180,\n",
    "    flip=True,\n",
    "    shear=False,\n",
    "    class_weight=None,\n",
    "    seed=train_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load an Existing Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell import model_zoo\n",
    "\n",
    "# Tracking model settings\n",
    "features = {'appearance', 'distance', 'neighborhood', 'regionprop'}\n",
    "min_track_length = 9\n",
    "neighborhood_scale_size=30\n",
    "batch_size = 128  \n",
    "crop_dim = 32\n",
    "in_shape = (crop_dim, crop_dim, 1)\n",
    "\n",
    "# Re-instantiate the tracking model\n",
    "tracking_model = model_zoo.siamese_model(\n",
    "    input_shape=in_shape,\n",
    "    neighborhood_scale_size=neighborhood_scale_size,\n",
    "    features=features)\n",
    "\n",
    "# Load model weights\n",
    "siamese_weights_file = 'tracking_model_seed1_tl9.h5'\n",
    "siamese_weights_file = os.path.join(MODEL_DIR, siamese_weights_file)\n",
    "\n",
    "tracking_model.load_weights(siamese_weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Model Performance with a Confusion Matrix - Requires a Seed Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/measure/_regionprops.py:250: UserWarning: regionprops and image moments (including moments, normalized moments, central moments, and inertia tensor) of 2D images will change from xy coordinates to rc coordinates in version 0.16.\n",
      "See https://scikit-image.org/docs/0.14.x/release_notes_and_installation.html#deprecations for details on how to avoid this message.\n",
      "  warn(XY_TO_RC_DEPRECATION_MESSAGE)\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/measure/_regionprops.py:260: UserWarning: regionprops and image moments (including moments, normalized moments, central moments, and inertia tensor) of 2D images will change from xy coordinates to rc coordinates in version 0.16.\n",
      "See https://scikit-image.org/docs/0.14.x/release_notes_and_installation.html#deprecations for details on how to avoid this message.\n",
      "  warn(XY_TO_RC_DEPRECATION_MESSAGE)\n"
     ]
    }
   ],
   "source": [
    "# Using DATA_FILE from above to extract Test Data\n",
    "\n",
    "# Import Statements\n",
    "import deepcell.image_generators as generators\n",
    "from deepcell.utils.data_utils import get_data\n",
    "\n",
    "# Get the data\n",
    "train_dict, test_dict = get_data(DATA_FILE, mode='siamese_daughters', seed=train_seed)\n",
    "\n",
    "# Build the generator and iterator\n",
    "datagen_test = generators.SiameseDataGenerator(\n",
    "        rotation_range=180,  # randomly rotate images by 0 to rotation_range degrees\n",
    "        shear_range=0,     # randomly shear images in the range (radians , -shear_range to shear_range)\n",
    "        horizontal_flip=1, # randomly flip images\n",
    "        vertical_flip=1)   # randomly flip images\n",
    "\n",
    "test_iterator = generators.SiameseIterator(test_dict,\n",
    "                                           datagen_test,\n",
    "                                           neighborhood_scale_size=neighborhood_scale_size,\n",
    "                                           min_track_length=min_track_length,\n",
    "                                           features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........\n",
      "[[10544     8     8]\n",
      " [   21 10484    32]\n",
      " [  353   714  9544]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y = []\n",
    "Y_pred = []\n",
    "for i in range(1,1000):\n",
    "    if i % 100 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "    lst, y_true = next(test_iterator)\n",
    "    y_true = np.argmax(y_true, axis=-1)\n",
    "    y_pred = np.argmax(tracking_model.predict(lst), axis=-1)\n",
    "    Y.append(y_true)\n",
    "    Y_pred.append(y_pred)\n",
    "    \n",
    "Y = np.concatenate(Y, axis=0)\n",
    "Y_pred = np.concatenate(Y_pred, axis=0)\n",
    "\n",
    "print(\"\")\n",
    "cm = confusion_matrix(Y, Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy across all three classes:  0.9641730793490602\n",
      "Accuracy for each individual class [Different, Same, Daughter]:  [0.99848485 0.99497011 0.89944397]\n"
     ]
    }
   ],
   "source": [
    "test_acc = sum(np.array(Y) == np.array(Y_pred)) / len(Y)\n",
    "print('Accuracy across all three classes: ', test_acc)\n",
    "\n",
    "# Normalize the diagonal entries of the confusion matrix\n",
    "cm = cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]\n",
    "# Diagonal entries are the accuracies of each class\n",
    "print('Accuracy for each individual class [Different, Same, Daughter]: ', cm.diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "This model is used within an assignment problem framework to track cells through time-lapse sequences and build cell lineages. To see how this works on example data, refer to Part 2 of this notebook series: [Tracking Example with Benchmarking](https://github.com/vanvalenlab/deepcell-tf/blob/master/scripts/tracking/Tracking%20Example%20with%20Benchmarking.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
