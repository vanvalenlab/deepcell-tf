{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install keras_retinanet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import errno\n",
    "import argparse\n",
    "import fnmatch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from skimage.external.tifffile import imsave\n",
    "from skimage.external.tifffile import TiffFile\n",
    "\n",
    "import deepcell\n",
    "from deepcell.utils.misc_utils import sorted_nicely\n",
    "from deepcell.utils.tracking_utils import load_trks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Multiple Movies with Multiple Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define File Locations and Load the Benchmark Data to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data to load (benchmark test folders)\n",
    "bens_trks_3T3  = '/data/track_data/test_DC_SEG/tracked/3T3/'\n",
    "bens_trks_HEK  = '/data/track_data/test_DC_SEG/tracked/HEK293/'\n",
    "bens_trks_HeLa = '/data/track_data/test_DC_SEG/tracked/HeLa/'\n",
    "bens_trks_RAW  = '/data/track_data/test_DC_SEG/tracked/RAW264.7/'\n",
    "\n",
    "ben_trks_folders = [bens_trks_3T3, bens_trks_HEK, bens_trks_HeLa, bens_trks_RAW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define destination folders for False Positive Corrected tracked batches\n",
    "cor_tracked_bens_3T3  = '/data/track_data/Final_Benchmarks/DC_TEST_DC_SEG/3T3/'\n",
    "cor_tracked_bens_HEK  = '/data/track_data/Final_Benchmarks/DC_TEST_DC_SEG/HEK293/'\n",
    "cor_tracked_bens_HeLa = '/data/track_data/Final_Benchmarks/DC_TEST_DC_SEG/HeLa/'\n",
    "cor_tracked_bens_RAW  = '/data/track_data/Final_Benchmarks/DC_TEST_DC_SEG/RAW264.7/'\n",
    "\n",
    "cor_trk_ben_folders = [cor_tracked_bens_3T3, cor_tracked_bens_HEK, cor_tracked_bens_HeLa, cor_tracked_bens_RAW]\n",
    "\n",
    "# Define a base file name for the output\n",
    "BASE_NAME = 'batch_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISBI data to load\n",
    "bens_trks_HeLa  = '/data/track_data/ISBI_Challenge/Fluo-N2DL-HeLa/ForBenchmarking/'\n",
    "#bens_trks_HeLa  = '/data/track_data/ISBI_Challenge_DC_SEG/tracked/HeLa/'\n",
    "\n",
    "ben_trks_folders = [bens_trks_HeLa]\n",
    "\n",
    "# ISBI destination folders for False Positive Corrected tracked batches\n",
    "cor_tracked_bens_HeLa  = '/data/track_data/Final_Benchmarks/ISBI/Fluo-N2DL-HeLa_noFPP/'\n",
    "#cor_tracked_bens_HeLa  = '/data/track_data/Final_Benchmarks/ISBI_DC_SEG/MaskRCNN/Fluo-N2DL-HeLa/'\n",
    "\n",
    "cor_trk_ben_folders = [cor_tracked_bens_HeLa]\n",
    "\n",
    "# Define a base file name for the output\n",
    "BASE_NAME = 'batch_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Through Each Dataset with Multiple Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import a Tracking Model to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell import model_zoo\n",
    "\n",
    "# Tracking model settings\n",
    "features = {'appearance', 'distance', 'neighborhood', 'regionprop'}\n",
    "min_track_length = 9\n",
    "neighborhood_scale_size = 30\n",
    "batch_size = 128  \n",
    "crop_dim = 32\n",
    "\n",
    "in_shape = (crop_dim, crop_dim, 1)\n",
    "\n",
    "# Re-instantiate the tracking model\n",
    "tracking_model = model_zoo.siamese_model(\n",
    "    input_shape=in_shape,\n",
    "    neighborhood_scale_size=neighborhood_scale_size,\n",
    "    features=features)\n",
    "\n",
    "# Load model weights\n",
    "MODEL_DIR = '/data/models'\n",
    "siamese_weights_file = 'tracking_model_benchmarking_757_step5_20epoch_80split_9tl.h5'\n",
    "siamese_weights_file = os.path.join(MODEL_DIR, siamese_weights_file)\n",
    "\n",
    "tracking_model.load_weights(siamese_weights_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Tracking Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Critical Parameters for Grid Search\n",
    "\n",
    "# If model prediction for daughter is higher than this parameter, then daughter assignment made\n",
    "#division=[0.5, 0.55, 0.6, 0.65, 0.7, 0.8, 0.9]\n",
    "division=[0.9]\n",
    "\n",
    "# Parameter for cost matrix - if other possible assignments are higher than this, then a birth occurred\n",
    "#birth=[0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "birth=[0.99]\n",
    "\n",
    "# Parameter for cost matrix - if other possible assignments are higher than this, then a death occurred\n",
    "#death=[0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "death=[0.99]\n",
    "\n",
    "# Define Parameters that will not be changed\n",
    "max_distance=50\n",
    "\n",
    "# Define Parameters that cannot be changed (because they are model dependent or training data dependent)\n",
    "track_length=9\n",
    "neighborhood_scale_size=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Model Iteratively for the Parameters of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking:  /data/track_data/ISBI_Challenge/Fluo-N2DL-HeLa/ForBenchmarking/batch_0.trk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/measure/_regionprops.py:250: UserWarning: regionprops and image moments (including moments, normalized moments, central moments, and inertia tensor) of 2D images will change from xy coordinates to rc coordinates in version 0.16.\n",
      "See https://scikit-image.org/docs/0.14.x/release_notes_and_installation.html#deprecations for details on how to avoid this message.\n",
      "  warn(XY_TO_RC_DEPRECATION_MESSAGE)\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/measure/_regionprops.py:260: UserWarning: regionprops and image moments (including moments, normalized moments, central moments, and inertia tensor) of 2D images will change from xy coordinates to rc coordinates in version 0.16.\n",
      "See https://scikit-image.org/docs/0.14.x/release_notes_and_installation.html#deprecations for details on how to avoid this message.\n",
      "  warn(XY_TO_RC_DEPRECATION_MESSAGE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking frame 1\n",
      "Tracked frame 1 in 6.6028862446546555 seconds.\n",
      "Tracking frame 2\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 2 in 6.679745081812143 seconds.\n",
      "Tracking frame 3\n",
      "Tracked frame 3 in 6.817187283188105 seconds.\n",
      "Tracking frame 4\n",
      "New track\n",
      "New track\n",
      "Tracked frame 4 in 6.975821662694216 seconds.\n",
      "Tracking frame 5\n",
      "Tracked frame 5 in 7.0617163479328156 seconds.\n",
      "Tracking frame 6\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Tracked frame 6 in 7.362284105271101 seconds.\n",
      "Tracking frame 7\n",
      "Tracked frame 7 in 7.578709091991186 seconds.\n",
      "Tracking frame 8\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "New track\n",
      "Tracked frame 8 in 7.630536511540413 seconds.\n",
      "Tracking frame 9\n",
      "Tracked frame 9 in 7.901914440095425 seconds.\n",
      "Tracking frame 10\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "New track\n",
      "Tracked frame 10 in 8.240629374980927 seconds.\n",
      "Tracking frame 11\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 11 in 8.596838343888521 seconds.\n",
      "Tracking frame 12\n",
      "New track\n",
      "Tracked frame 12 in 9.040392022579908 seconds.\n",
      "Tracking frame 13\n",
      "Tracked frame 13 in 9.149737916886806 seconds.\n",
      "Tracking frame 14\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Tracked frame 14 in 9.43424280360341 seconds.\n",
      "Tracking frame 15\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 15 in 9.821655061095953 seconds.\n",
      "Tracking frame 16\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 16 in 10.110387682914734 seconds.\n",
      "Tracking frame 17\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Tracked frame 17 in 10.444054458290339 seconds.\n",
      "Tracking frame 18\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 18 in 11.228910241276026 seconds.\n",
      "Tracking frame 19\n",
      "New track\n",
      "Tracked frame 19 in 11.568690359592438 seconds.\n",
      "Tracking frame 20\n",
      "Tracked frame 20 in 11.561698731034994 seconds.\n",
      "Tracking frame 21\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 21 in 11.887657210230827 seconds.\n",
      "Tracking frame 22\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 22 in 12.178868692368269 seconds.\n",
      "Tracking frame 23\n",
      "New track\n",
      "New track\n",
      "Tracked frame 23 in 12.781797092407942 seconds.\n",
      "Tracking frame 24\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Tracked frame 24 in 13.175726104527712 seconds.\n",
      "Tracking frame 25\n",
      "New track\n",
      "New track\n",
      "Tracked frame 25 in 13.47976015880704 seconds.\n",
      "Tracking frame 26\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "New track\n",
      "Tracked frame 26 in 13.671932969242334 seconds.\n",
      "Tracking frame 27\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 27 in 14.178974330425262 seconds.\n",
      "Tracking frame 28\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 28 in 14.462543956935406 seconds.\n",
      "Tracking frame 29\n",
      "Tracked frame 29 in 14.701960809528828 seconds.\n",
      "Tracking frame 30\n",
      "New track\n",
      "New track\n",
      "Tracked frame 30 in 14.871473986655474 seconds.\n",
      "Tracking frame 31\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 31 in 15.082330476492643 seconds.\n",
      "Tracking frame 32\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 32 in 15.4085105471313 seconds.\n",
      "Tracking frame 33\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 33 in 15.932533096522093 seconds.\n",
      "Tracking frame 34\n",
      "New track\n",
      "New track\n",
      "New track\n",
      "Tracked frame 34 in 16.09079009667039 seconds.\n",
      "Tracking frame 35\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "New track\n",
      "Tracked frame 35 in 16.41284627094865 seconds.\n",
      "Tracking frame 36\n",
      "New track\n",
      "Tracked frame 36 in 16.580837544053793 seconds.\n",
      "Tracking frame 37\n",
      "New track\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 37 in 16.631249248981476 seconds.\n",
      "Tracking frame 38\n",
      "New track\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 38 in 16.786951832473278 seconds.\n",
      "Tracking frame 39\n",
      "Tracked frame 39 in 17.372205805033445 seconds.\n",
      "Tracking frame 40\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 40 in 17.930009730160236 seconds.\n",
      "Tracking frame 41\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 41 in 17.874016027897596 seconds.\n",
      "Tracking frame 42\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 42 in 18.66776079684496 seconds.\n",
      "Tracking frame 43\n",
      "New track\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "New track\n",
      "Tracked frame 43 in 19.21862083300948 seconds.\n",
      "Tracking frame 44\n",
      "New track\n",
      "Tracked frame 44 in 20.081795752048492 seconds.\n",
      "Tracking frame 45\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Tracked frame 45 in 20.764600820839405 seconds.\n",
      "Tracking frame 46\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 46 in 21.0103910677135 seconds.\n",
      "Tracking frame 47\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 47 in 21.227341920137405 seconds.\n",
      "Tracking frame 48\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 48 in 21.42565067484975 seconds.\n",
      "Tracking frame 49\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 49 in 21.750551030039787 seconds.\n",
      "Tracking frame 50\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "Tracked frame 50 in 22.155727371573448 seconds.\n",
      "Tracking frame 51\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "New track\n",
      "Division detected\n",
      "New track\n",
      "Tracked frame 51 in 22.175054006278515 seconds.\n",
      "Tracking frame 52\n"
     ]
    }
   ],
   "source": [
    "# The tracking model is used in concert with other processes to track cells\n",
    "# Import the neccesary tracking functionality\n",
    "import deepcell.tracking\n",
    "\n",
    "# The tracker requires normalized images as input\n",
    "def image_norm(original_image):\n",
    "    # NNs prefer input data that is 0 mean and unit variance\n",
    "    normed_image = (original_image - np.mean(original_image)) / np.std(original_image)\n",
    "    return normed_image\n",
    "\n",
    "# Go through each Dataset (3T3, HEK293, HeLa, RAW264.7)\n",
    "for set_num, dataset in enumerate(ben_trks_folders):\n",
    "    # Go through each batch (movie) in each dataset\n",
    "    movie_list = sorted_nicely(os.listdir(dataset))\n",
    "    for batch_num, movie in enumerate(movie_list):\n",
    "        # Load the trk file       \n",
    "        filename = os.path.join(dataset, movie)\n",
    "        trks = load_trks(filename)\n",
    "        lineages, raw, tracked = trks[\"lineages\"], trks[\"X\"], trks[\"y\"]\n",
    "        \n",
    "        # Normalize raw images\n",
    "        for frame in range(raw.shape[0]):\n",
    "            raw[frame, :, :, 0] = image_norm(raw[frame, :, :, 0]) \n",
    "\n",
    "        # Track each movie with each division parameter\n",
    "        print('Tracking: ', filename)\n",
    "        for div_param in division:\n",
    "            # For each birth parameter\n",
    "            for birth_param in birth:\n",
    "                # For each death parameter\n",
    "                for death_param in death:\n",
    "                    # Track with the selected parameters\n",
    "                    trial = deepcell.tracking.cell_tracker(raw, tracked,\n",
    "                                         tracking_model, max_distance=max_distance, track_length=track_length, \n",
    "                                         division=div_param, birth=birth_param, death=death_param,\n",
    "                                         neighborhood_scale_size=neighborhood_scale_size,\n",
    "                                         features=features)\n",
    "                    trial._track_cells()\n",
    "                    # Run FP post processing and save the result in the correct location\n",
    "                    file_name = BASE_NAME + str(batch_num).zfill(3)+'_div'+str(div_param).zfill(3)+'_b'+str(birth_param).zfill(3)+'_d'+str(death_param).zfill(3)+'.trk'\n",
    "                    file_path = os.path.join(cor_trk_ben_folders[set_num], file_name)\n",
    "\n",
    "                    #fp_fixed_trk = trial.postprocess(file_path)\n",
    "                    trial.dump(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate the Results Into ISBI Format for Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ISBI Translator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds a new track to the lineage and swap the labels accordingly in the images\n",
    "def create_new_ISBI_track(batch_tracked, batch_info, old_label, frames, daughters, frame_div):\n",
    "    \n",
    "    new_track = max(batch_info.keys())\n",
    "    new_label = new_track + 1\n",
    "         \n",
    "    batch_info[new_label] = {}\n",
    "    batch_info[new_label]['old_label'] = old_label\n",
    "    batch_info[new_label]['label'] = new_label\n",
    "\n",
    "    batch_info[new_label]['frames'] = frames\n",
    "    batch_info[new_label]['daughters'] = daughters\n",
    "    batch_info[new_label]['frame_div'] = frame_div\n",
    "    batch_info[new_label]['parent'] = None\n",
    "\n",
    "    for frame in frames:\n",
    "        batch_tracked[frame][batch_tracked[frame] == old_label] = new_label\n",
    "        \n",
    "    return batch_info, batch_tracked\n",
    "\n",
    "# Check for contiguous tracks (tracks should only consist of consecutive tracks)\n",
    "# Split one track into two if neccesary\n",
    "def contig_tracks(label, batch_info, batch_tracked):\n",
    "    \n",
    "    frame_div_missing = False\n",
    "    \n",
    "    original_label = label\n",
    "    frames = batch_info[original_label]['frames']\n",
    "    final_frame_idx = len(frames) - 1\n",
    "       \n",
    "    for frame_idx, frame in enumerate(frames):\n",
    "        next_con_frame = frame + 1\n",
    "        # If the next frame is available and contiguous we should move on to the next frame. Otherwise:\n",
    "        # If the next frame is available and NONcontiguous we should separate this track into two \n",
    "        if frame_idx + 1 <= final_frame_idx and next_con_frame != frames[frame_idx + 1]:\n",
    "            contig_end_idx = frame_idx\n",
    "\n",
    "            next_trk_frames = frames[frame_idx+1:]\n",
    "            daughters = batch_info[original_label]['daughters']\n",
    "            \n",
    "            \n",
    "            if 'frame_div' in batch_info[original_label]:\n",
    "                frame_div = batch_info[original_label]['frame_div']\n",
    "            else:\n",
    "                frame_div = None\n",
    "                frame_div_missing = True\n",
    "                                  \n",
    "            # Create a new track to hold the information from this frame forward and add it to the batch\n",
    "            batch_info, batch_tracked = create_new_ISBI_track(batch_tracked, batch_info, original_label, \n",
    "                                                                next_trk_frames, daughters, frame_div)\n",
    "                        \n",
    "            # Adjust the info for the current track to vacate the new track info\n",
    "            batch_info[original_label]['frames'] = frames[0:contig_end_idx+1]\n",
    "            batch_info[original_label]['daughters'] = []\n",
    "            batch_info[original_label]['frame_div'] = None\n",
    "                        \n",
    "            # Because we are splitting tracks recursively, we stop here\n",
    "            break\n",
    "        \n",
    "        # If the current frame is the last frame then were done\n",
    "        # Either the last frame is contiguous and we don't alter batch_info\n",
    "        # or it's not and it's been made into a new track by the previous iteration of the loop\n",
    "        \n",
    "    # Print warning if there is no 'frame_div'\n",
    "    if frame_div_missing:\n",
    "        print('Warning: frame_div is missing')\n",
    "    \n",
    "    return batch_info, batch_tracked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Save Location for Benchmark Files (ie: 001, 001_RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where benchmark data will be saved\n",
    "BENCH_DIR = '/data/tracking_benchmarks/Final_Benchmarks/DC_TEST_DC_SEG'\n",
    "\n",
    "# Define where tracks will be saved\n",
    "bench_3T3  = os.path.join(BENCH_DIR, '3T3/')\n",
    "bench_HEK  = os.path.join(BENCH_DIR, 'HEK293/')\n",
    "bench_HeLa = os.path.join(BENCH_DIR, 'HeLa/')\n",
    "bench_RAW  = os.path.join(BENCH_DIR, 'RAW264.7/')\n",
    "\n",
    "bench_tracked_folders = [bench_3T3, bench_HEK, bench_HeLa, bench_RAW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ISBI\n",
    "# Define alt location where benchmark data will be saved\n",
    "BENCH_DIR = '/data/tracking_benchmarks/Final_Benchmarks/ISBI/'\n",
    "#BENCH_DIR = '/data/tracking_benchmarks/Final_Benchmarks/ISBI_DC_SEG/MaskRCNN/'\n",
    "\n",
    "# Define where tracks will be saved\n",
    "bench_HeLa = os.path.join(BENCH_DIR, 'Fluo-N2DL-HeLa_noFPP')\n",
    "#bench_HeLa = os.path.join(BENCH_DIR, 'Fluo-N2DL-HeLa/')\n",
    "\n",
    "bench_tracked_folders = [bench_HeLa]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate All trk files and Generate Benchmark Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create new benchmark files (ie: 001_RES) from mutliple .trk files\n",
    "## NB: RAW and GT Files Should Already Exist\n",
    "\n",
    "\n",
    "# Go through each Dataset (3T3, HEK293, HeLa, RAW264.7)\n",
    "for set_num, dataset in enumerate(cor_trk_ben_folders):\n",
    "    # Go through each batch (movie) in each dataset\n",
    "    movie_list = sorted_nicely(os.listdir(dataset))\n",
    "    \n",
    "    for batch_num, batch in enumerate(movie_list):\n",
    "\n",
    "        # Load the trk file\n",
    "        filename = os.path.join(dataset, batch)\n",
    "        trks = load_trks(filename)\n",
    "        lineages, raw, tracked = trks[\"lineages\"], trks[\"X\"], trks[\"y\"]\n",
    "\n",
    "        # Define Save Location\n",
    "        PARAMS = 'div'+str(div_param).zfill(3)+'_b'+str(birth_param).zfill(3)+'_d'+str(death_param).zfill(3)\n",
    "        \n",
    "        # Build subdirectories to hold benchmark info           \n",
    "        BENCHMARK_DIR = os.path.abspath(os.path.join(bench_tracked_folders[set_num], PARAMS))\n",
    "        B_SUB_DIR = os.path.join(BENCHMARK_DIR, '{:03}_RES'.format(batch_num+1))\n",
    "\n",
    "        print('Files will be saved at: ', B_SUB_DIR)\n",
    "\n",
    "        # create directories if they do not exist\n",
    "        try:\n",
    "            os.makedirs(B_SUB_DIR)\n",
    "        except OSError as exc:  # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "        # Record lineage data in txt as it is generated\n",
    "        batch_info = lineages[0]\n",
    "\n",
    "        # Prepare output txt\n",
    "        text_file = open(os.path.join(B_SUB_DIR, \"res_track.txt\"), \"w\")\n",
    "\n",
    "        batch_tracked = tracked\n",
    "        labels = list(batch_info.keys())\n",
    "        max_label = max(labels)\n",
    "\n",
    "        labels_to_remove = []\n",
    "        for label in labels:\n",
    "\n",
    "            batch_info, batch_tracked = contig_tracks(label, batch_info, batch_tracked)\n",
    "\n",
    "            first_frame = np.amin(batch_info[label]['frames'])          \n",
    "            last_frame = np.amax(batch_info[label]['frames'])\n",
    "            if batch_info[label]['parent']:\n",
    "                parent = batch_info[label]['parent']\n",
    "            else:\n",
    "                parent = 0\n",
    "\n",
    "            print(label, first_frame, last_frame, parent)\n",
    "            text_file.write('{} {} {} {}\\n'.format(label, first_frame, last_frame, parent))\n",
    "\n",
    "            # Check if the track need to be split\n",
    "            if max(batch_info.keys()) > max_label:\n",
    "                # If so, a new track was added\n",
    "                new_max_label = max(batch_info.keys())\n",
    "                labels.append(new_max_label)\n",
    "                max_label = new_max_label\n",
    "\n",
    "        text_file.close()\n",
    "        lineages[0] = batch_info\n",
    "        tracked = batch_tracked\n",
    "\n",
    "\n",
    "        # Save Image Files\n",
    "\n",
    "        channel = 0 # These images should only have one channel\n",
    "        for i in range(raw.shape[0]):\n",
    "            #name_raw = os.path.join(B_SUB_DIR_RAW,'t{:03}_.tif'.format(i))\n",
    "            name_tracked = os.path.join(B_SUB_DIR,'mask{:03}.tif'.format(i))\n",
    "            #imsave(name_raw, raw[i, :, :, channel])\n",
    "            imsave(name_tracked, tracked[i, :, :, channel].astype('uint16'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate/Copy RAW and GT Benchmark Files to the Appropriate Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, errno\n",
    "\n",
    "def copy_raw_folders(src, dst):\n",
    "    dir_name = os.path.basename(src)\n",
    "    dst_path = os.path.join(dst, dir_name)\n",
    "    \n",
    "    # create directories if they do not exist\n",
    "    try:\n",
    "        os.makedirs(dst_path)\n",
    "    except OSError as exc:  # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "    # copy each file over\n",
    "    file_list = os.listdir(src)\n",
    "    for src_file in file_list:\n",
    "        src_path = os.path.join(src, src_file)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "        \n",
    "        \n",
    "def copy_gt_folders(src, dst):\n",
    "    \n",
    "    dir_name = os.path.basename(src)\n",
    "    dst_path_SEG = os.path.join(dst, dir_name, 'SEG')\n",
    "    dst_path_TRA = os.path.join(dst, dir_name, 'TRA')\n",
    "    \n",
    "    # create directories if they do not exist\n",
    "    for d in (dst_path_SEG, dst_path_TRA):\n",
    "        try:\n",
    "            os.makedirs(d)\n",
    "        except OSError as exc:  # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "\n",
    "    # copy each file over\n",
    "    file_list = os.listdir(os.path.join(src, 'SEG'))\n",
    "    for src_file in file_list:\n",
    "        src_path = os.path.join(src, 'SEG', src_file)\n",
    "        shutil.copy(src_path, dst_path_SEG)\n",
    "        \n",
    "    file_list = os.listdir(os.path.join(src, 'TRA'))\n",
    "    for src_file in file_list:\n",
    "        src_path = os.path.join(src, 'TRA', src_file)\n",
    "        shutil.copy(src_path, dst_path_TRA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create/Copy benchmark files (ie: 001, 001_GT, etc) from mutliple .trk files\n",
    "import shutil\n",
    "\n",
    "# Define all the original RAW/GT Benchmark Folders\n",
    "# bens_folders_3T3  = '/data/tracking_benchmarks/Viterbi_test/GT_Tracking/3T3/NIH/'\n",
    "# bens_folders_HEK  = '/data/tracking_benchmarks/Viterbi_test/GT_Tracking/HEK293/generic/'\n",
    "# bens_folders_HeLa = '/data/tracking_benchmarks/Viterbi_test/GT_Tracking/HeLa/S3/'\n",
    "# bens_folders_RAW  = '/data/tracking_benchmarks/Viterbi_test/GT_Tracking/RAW264.7/generic/'\n",
    "\n",
    "# bens_folders_list = [bens_folders_3T3, bens_folders_HEK, bens_folders_HeLa, bens_folders_RAW]\n",
    "\n",
    "# ISBI\n",
    "# Define all the original RAW/GT Benchmark Folders\n",
    "bens_folders_HeLa = '/data/tracking_benchmarks/Viterbi_ISBI/GT_Tracking/Fluo-N2DL-HeLa'\n",
    "\n",
    "bens_folders_list = [bens_folders_HeLa]\n",
    "\n",
    "\n",
    "# Go through each Dataset (3T3, HEK293, HeLa, RAW264.7)\n",
    "for ben_idx, ben_folder in enumerate(bens_folders_list):\n",
    "    folder_list = os.listdir(ben_folder)\n",
    "    folder_list_sorted = sorted_nicely(folder_list)\n",
    "    \n",
    "    # Build two lists - 1 that holds all the RAW batches and 1 that holds all GT batches\n",
    "    # These will be the source files to copy\n",
    "    RAW_folders = []\n",
    "    GT_folders = []\n",
    "    # Loop through all of the folders in the directory\n",
    "    for folder in folder_list_sorted:\n",
    "        # First select folders containing raw images\n",
    "        if fnmatch.fnmatch(folder, '???'):\n",
    "            raw_folder = os.path.join(ben_folder,folder)\n",
    "            RAW_folders.append(raw_folder)\n",
    "        # Next select folders containing GT images\n",
    "        elif fnmatch.fnmatch(folder, '???_GT'):\n",
    "            GT_folder = os.path.join(ben_folder,folder)\n",
    "            GT_folders.append(GT_folder)\n",
    "            \n",
    "    # Copy the raw and GT files for each batch (movie) in each dataset\n",
    "    for src_raw, src_gt in zip(RAW_folders, GT_folders):\n",
    "                \n",
    "        # Define the destination folders (1 for each paramter combination)\n",
    "        # Loop through each of these folders and copy\n",
    "        for div_param in division:\n",
    "            # For each birth parameter\n",
    "            for birth_param in birth:\n",
    "                # For each death parameter\n",
    "                for death_param in death:\n",
    "                                        \n",
    "                    PARAMS = 'div'+str(div_param)+'_b'+str(birth_param)+'_d'+str(death_param)\n",
    "                    BENCHMARK_DIR = os.path.join(bench_tracked_folders[ben_idx], PARAMS)\n",
    "                    dest = BENCHMARK_DIR\n",
    " \n",
    "                    copy_raw_folders(src_raw, dest)\n",
    "                    copy_gt_folders(src_gt, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Graph Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Comparison Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from skimage.measure import regionprops\n",
    "from skimage.external.tifffile import TiffFile\n",
    "from deepcell.utils.compute_overlap import compute_overlap\n",
    " \n",
    "    \n",
    "\n",
    "def create_graph(file, node_key=None):\n",
    "    df = pd.read_csv(file, header=None, sep=' ', names=['Cell_ID','Start','End','Parent_ID'])\n",
    "    \n",
    "    if node_key != None:\n",
    "        df[['Cell_ID','Parent_ID']] = df[['Cell_ID','Parent_ID']].replace(node_key)    \n",
    "    \n",
    "    edges = pd.DataFrame()    \n",
    "    \n",
    "    # Add each cell lineage as a set of edges to df    \n",
    "    for _, row in df.iterrows():\n",
    "        tpoints = np.arange(row['Start'], row['End']+1)\n",
    "        deltaT = len(tpoints)        \n",
    "        \n",
    "        cellid = ['{cellid}_{frame}'.format(cellid = row['Cell_ID'], frame = t) for t in tpoints]\n",
    "        source = cellid[0:-1]\n",
    "        target = cellid[1:]        \n",
    "        \n",
    "        edges = edges.append(pd.DataFrame({'source':source, 'target':target}))    \n",
    "        \n",
    "    Dattr = {}\n",
    "    # Add parent-daughter connections\n",
    "    for _,row in df[df['Parent_ID']!=0].iterrows():\n",
    "        source = '{cellid}_{frame}'.format(cellid = row['Parent_ID'], frame = row['Start']-1)\n",
    "        target = '{cellid}_{frame}'.format(cellid = row['Cell_ID'], frame = row['Start'])        \n",
    "        \n",
    "        edges = edges.append(pd.DataFrame({'source':[source], 'target':[target]}))\n",
    "        \n",
    "        Dattr[source] = {'division':True}    \n",
    "        \n",
    "    # Create graph\n",
    "    G = nx.from_pandas_edgelist(edges, source='source', target='target')\n",
    "    nx.set_node_attributes(G, Dattr)\n",
    "    return G\n",
    "\n",
    "def load_data(pattern):\n",
    "    files = np.sort(glob.glob(pattern))\n",
    "    Lim = []\n",
    "    for i,f in enumerate(files):\n",
    "        Lim.append(TiffFile(f).asarray())    \n",
    "        \n",
    "    im = np.stack(Lim)\n",
    "    return(im)\n",
    "\n",
    "\n",
    "\n",
    "def match_nodes(pattern1, pattern2):\n",
    "    gt = load_data(pattern1)\n",
    "    res = load_data(pattern2)\n",
    "    \n",
    "    num_frames = gt.shape[0]\n",
    "    iou = np.zeros((num_frames, np.max(gt)+1, np.max(res)+1))   \n",
    "     \n",
    "    # Compute IOUs only when neccesary\n",
    "    # If bboxs for true and pred do not overlap with each other, the assignment is immediate\n",
    "    # Otherwise use pixel-wise IOU to determine which cell is which\n",
    "    \n",
    "    # Regionprops expects one frame at a time\n",
    "    for frame in range(num_frames):\n",
    "        gt_frame = gt[frame]\n",
    "        res_frame = res[frame]\n",
    "        \n",
    "        gt_props = regionprops(np.squeeze(gt_frame.astype('int')))\n",
    "        gt_boxes = [np.array(gt_prop.bbox) for gt_prop in gt_props]\n",
    "        gt_boxes = np.array(gt_boxes).astype('double')\n",
    "        gt_box_labels = [int(gt_prop.label) for gt_prop in gt_props]\n",
    "        \n",
    "        res_props = regionprops(np.squeeze(res_frame.astype('int')))\n",
    "        res_boxes = [np.array(res_prop.bbox) for res_prop in res_props]\n",
    "        res_boxes = np.array(res_boxes).astype('double')\n",
    "        res_box_labels = [int(res_prop.label) for res_prop in res_props]\n",
    "        \n",
    "        overlaps = compute_overlap(gt_boxes, res_boxes)    # has the form [gt_bbox, res_bbox]\n",
    "        \n",
    "        # Find the bboxes that have overlap at all (ind_ corresponds to box number - starting at 0)\n",
    "        ind_gt, ind_res = np.nonzero(overlaps)\n",
    "               \n",
    "        #frame_ious = np.zeros(overlaps.shape)\n",
    "        for index in range(ind_gt.shape[0]):\n",
    "                        \n",
    "            iou_gt_idx = gt_box_labels[ind_gt[index]]\n",
    "            iou_res_idx = res_box_labels[ind_res[index]]\n",
    "            intersection = np.logical_and(gt_frame==iou_gt_idx, res_frame==iou_res_idx)\n",
    "            union = np.logical_or(gt_frame==iou_gt_idx, res_frame==iou_res_idx)            \n",
    "            iou[frame, iou_gt_idx, iou_res_idx] = intersection.sum() / union.sum()\n",
    "                \n",
    "    gtcells, rescells = np.where(np.nansum(iou,axis=0)>=1)\n",
    "    \n",
    "    \n",
    "    return gtcells, rescells\n",
    "\n",
    "def classify_divisions(G_gt,G_res):    \n",
    "    \n",
    "    # Identify nodes with parent attribute\n",
    "    div_gt =[node for node,d in G_gt.node.data() if d.get('division',False) == True]\n",
    "    div_res =[node for node,d in G_res.node.data() if d.get('division',False) == True]    \n",
    "    \n",
    "    divI = 0 # Correct division\n",
    "    divJ = 0 # Wrong division\n",
    "    divC = 0 # False positive division\n",
    "    divGH = 0 # Missed division    \n",
    "    \n",
    "    for node in div_gt:\n",
    "        nb_gt = list(G_gt.neighbors(node))        \n",
    "        \n",
    "        # Check if res node was also called a division\n",
    "        if node in div_res:\n",
    "            nb_res = list(G_gt.neighbors(node))\n",
    "            # If neighbors are same, then correct division\n",
    "            if Counter(nb_gt) == Counter(nb_res):\n",
    "                divI += 1\n",
    "            # Wrong division\n",
    "            elif len(nb_res) == 3:\n",
    "                divJ += 1\n",
    "            else:\n",
    "                divGH += 1\n",
    "        # If not called division, then missed division\n",
    "        else:\n",
    "            divGH += 1        \n",
    "            \n",
    "        # Remove processed nodes from res list\n",
    "        try:\n",
    "            div_res.remove(node)\n",
    "        except:\n",
    "            print('attempted removal of node {} failed'.format(node))    \n",
    "            \n",
    "    # Count any remaining res nodes as false positives   \n",
    "    divC += len(div_res)   \n",
    "    \n",
    "    return({'Correct division': divI,\n",
    "            'Incorrect division': divJ,\n",
    "            'False positive division':divC,\n",
    "            'False negative division':divGH})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteratively Compare Each Movie (Tracked with Each Parameter) To the Ground Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#####\n",
    "#####        LOAD/CHANGE BENCHMARK DIRECTORIES IF NECESSARY\n",
    "#####\n",
    "#############################################################################\n",
    "\n",
    "# Define where benchmark data will be saved\n",
    "#BENCH_DIR = '/data/tracking_benchmarks/DC_Seg_Test'\n",
    "#BENCH_DIR = '/data/tracking_benchmarks/DC_test_final_param_FP_cor/'\n",
    "#BENCH_DIR = '/data/tracking_benchmarks/DC_test_param_sol_FP_post/'\n",
    "BENCH_DIR = '/data/tracking_benchmarks/MORDOR_TEST/OldAcc/'\n",
    "#BENCH_DIR = '/data/tracking_benchmarks/Viterbi_test/KTH_Seg_Track'\n",
    "\n",
    "# Define where tracks will be saved\n",
    "# bench_3T3  = os.path.join(BENCH_DIR, '3T3/NIH')\n",
    "# bench_HEK  = os.path.join(BENCH_DIR, 'HEK293/generic')\n",
    "# bench_HeLa = os.path.join(BENCH_DIR, 'HeLa/S3')\n",
    "# bench_RAW  = os.path.join(BENCH_DIR, 'RAW264.7/generic')\n",
    "\n",
    "bench_3T3  = os.path.join(BENCH_DIR, '3T3/')\n",
    "bench_HEK  = os.path.join(BENCH_DIR, 'HEK293/')\n",
    "bench_HeLa = os.path.join(BENCH_DIR, 'HeLa/')\n",
    "bench_RAW  = os.path.join(BENCH_DIR, 'RAW264.7/')\n",
    "\n",
    "bench_tracked_folders = [bench_3T3, bench_HEK, bench_HeLa, bench_RAW]\n",
    "\n",
    "#BENCH_DIR = '/data/tracking_benchmarks/Viterbi_ISBI/DL_Tracking'\n",
    "\n",
    "# Define where tracks will be saved\n",
    "#bench_MSC  = os.path.join(BENCH_DIR, 'Fluo-C2DL-MSC')\n",
    "#bench_HeLa  = os.path.join(BENCH_DIR, 'Fluo-N2DL-HeLa')\n",
    "#bench_U373 = os.path.join(BENCH_DIR, 'PhC-C2DH-U373')\n",
    "\n",
    "#bench_tracked_folders = [bench_MSC, bench_HeLa, bench_U373]\n",
    "#bench_tracked_folders = [bench_HeLa]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempted removal of node 31_20 failed\n",
      "attempted removal of node 173_73 failed\n",
      "attempted removal of node 322_62 failed\n",
      "attempted removal of node 124_55 failed\n",
      "attempted removal of node 6_29 failed\n",
      "attempted removal of node 29_13 failed\n",
      "attempted removal of node 50_20 failed\n",
      "attempted removal of node 132_60 failed\n",
      "attempted removal of node 118_49 failed\n",
      "attempted removal of node 22_51 failed\n",
      "attempted removal of node 15_10 failed\n",
      "attempted removal of node 119_67 failed\n",
      "attempted removal of node 206_86 failed\n",
      "attempted removal of node 11_42 failed\n",
      "attempted removal of node 20_36 failed\n",
      "attempted removal of node 474_59 failed\n",
      "attempted removal of node 8_75 failed\n",
      "attempted removal of node 66_34 failed\n",
      "attempted removal of node 43_9 failed\n",
      "attempted removal of node 495_63 failed\n",
      "attempted removal of node 185_78 failed\n",
      "attempted removal of node 48_29 failed\n",
      "attempted removal of node 150_33 failed\n",
      "attempted removal of node 26_9 failed\n",
      "attempted removal of node 53_24 failed\n",
      "attempted removal of node 62_3 failed\n",
      "attempted removal of node 1_10 failed\n",
      "attempted removal of node 79_77 failed\n",
      "attempted removal of node 88_76 failed\n",
      "attempted removal of node 51_22 failed\n",
      "attempted removal of node 41_25 failed\n",
      "attempted removal of node 109_51 failed\n",
      "attempted removal of node 279_61 failed\n",
      "attempted removal of node 496_75 failed\n",
      "attempted removal of node 7_16 failed\n",
      "attempted removal of node 157_73 failed\n",
      "attempted removal of node 24_5 failed\n",
      "attempted removal of node 86_59 failed\n",
      "attempted removal of node 36_42 failed\n",
      "attempted removal of node 331_69 failed\n",
      "attempted removal of node 184_80 failed\n",
      "attempted removal of node 35_42 failed\n",
      "attempted removal of node 103_56 failed\n",
      "attempted removal of node 44_22 failed\n",
      "attempted removal of node 4_3 failed\n",
      "attempted removal of node 167_82 failed\n",
      "attempted removal of node 56_58 failed\n",
      "attempted removal of node 98_43 failed\n",
      "attempted removal of node 175_89 failed\n",
      "attempted removal of node 198_85 failed\n",
      "attempted removal of node 49_44 failed\n",
      "attempted removal of node 70_40 failed\n",
      "attempted removal of node 23_23 failed\n",
      "attempted removal of node 2_37 failed\n",
      "attempted removal of node 127_60 failed\n",
      "attempted removal of node 131_63 failed\n",
      "attempted removal of node 212_44 failed\n",
      "attempted removal of node 280_63 failed\n",
      "attempted removal of node 140_65 failed\n",
      "attempted removal of node 159_81 failed\n",
      "attempted removal of node 45_25 failed\n",
      "attempted removal of node 355_84 failed\n",
      "attempted removal of node 21_33 failed\n",
      "attempted removal of node 12_41 failed\n",
      "attempted removal of node 105_48 failed\n",
      "attempted removal of node 524_90 failed\n",
      "attempted removal of node 130_76 failed\n",
      "attempted removal of node 59_17 failed\n",
      "attempted removal of node 108_53 failed\n",
      "attempted removal of node 213_65 failed\n",
      "attempted removal of node 115_54 failed\n",
      "attempted removal of node 218_26 failed\n",
      "attempted removal of node 231_32 failed\n",
      "attempted removal of node 106_85 failed\n",
      "attempted removal of node 22_0 failed\n",
      "attempted removal of node 243_87 failed\n",
      "attempted removal of node 87_55 failed\n",
      "attempted removal of node 260_80 failed\n",
      "attempted removal of node 12_0 failed\n",
      "attempted removal of node 133_73 failed\n",
      "attempted removal of node 92_1 failed\n",
      "attempted removal of node 59_3 failed\n",
      "attempted removal of node 40_15 failed\n",
      "attempted removal of node 193_55 failed\n",
      "attempted removal of node 41_25 failed\n",
      "attempted removal of node 75_0 failed\n",
      "attempted removal of node 31_15 failed\n",
      "attempted removal of node 70_13 failed\n",
      "attempted removal of node 102_57 failed\n",
      "attempted removal of node 460_59 failed\n",
      "attempted removal of node 9_29 failed\n",
      "attempted removal of node 28_12 failed\n",
      "attempted removal of node 84_1 failed\n",
      "attempted removal of node 51_31 failed\n",
      "attempted removal of node 47_21 failed\n",
      "attempted removal of node 1_31 failed\n",
      "attempted removal of node 29_26 failed\n",
      "attempted removal of node 254_41 failed\n",
      "attempted removal of node 3_83 failed\n",
      "attempted removal of node 73_0 failed\n",
      "attempted removal of node 331_79 failed\n",
      "attempted removal of node 115_55 failed\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "#####\n",
    "#####       FOR MULTIPLE PARAMETERS:\n",
    "#####\n",
    "\n",
    "# Prep a list to hold each dataset's results\n",
    "dataset_cm = []\n",
    "\n",
    "# Go through each Dataset (3T3, HEK293, HeLa, RAW264.7)\n",
    "for ben_trk_folder in bench_tracked_folders:\n",
    "\n",
    "    # Prep a list to hold results for each set of parameters\n",
    "    params_cm = []\n",
    "   \n",
    "    # Go through each parameter combination\n",
    "    for div_param in division:\n",
    "        # For each birth parameter\n",
    "        for birth_param in birth:\n",
    "            # For each death parameter\n",
    "            for death_param in death:  \n",
    "\n",
    "                PARAMS = 'div'+str(div_param)+'_b'+str(birth_param)+'_d'+str(death_param)\n",
    "                BENCHMARK_DIR = os.path.join(ben_trk_folder, PARAMS)\n",
    "                \n",
    "                # Compile a list of each movie\n",
    "                sub_dirs = sorted_nicely(os.listdir(BENCHMARK_DIR))\n",
    "                movie_list = fnmatch.filter(sub_dirs, '???')\n",
    "                # Prep a list to hold each movie's results\n",
    "                cm_list = []\n",
    "                # Loop through each set of movies \n",
    "                for name in movie_list:\n",
    "                    # Extract track.txt for each movie\n",
    "                    pattern_gt = os.path.join(BENCHMARK_DIR, name + '_GT/TRA/')\n",
    "                    pattern_res = os.path.join(BENCHMARK_DIR, name + '_RES/')\n",
    "\n",
    "                    gtcells, rescells = match_nodes(pattern_gt+'*.tif',pattern_res+'*.tif')\n",
    "\n",
    "                    if len(np.unique(rescells)) < len(np.unique(gtcells)):\n",
    "                        node_key = {r:g for g,r in zip(gtcells,rescells)}\n",
    "                        # node_key\n",
    "                        # Maps gt nodes onto resnodes so must be applied to gt\n",
    "                        G_res = create_graph(pattern_res+'res_track.txt',node_key=node_key)\n",
    "                        G_gt = create_graph(pattern_gt+'man_track.txt')\n",
    "                        cm_list.append(classify_divisions(G_gt,G_res))\n",
    "                    else:\n",
    "                        node_key = {g:r for g,r in zip(gtcells,rescells)}\n",
    "                        G_res = create_graph(pattern_res+'res_track.txt')\n",
    "                        G_gt = create_graph(pattern_gt+'man_track.txt',node_key=node_key)\n",
    "                        cm_list.append(classify_divisions(G_gt,G_res))            \n",
    "\n",
    "                params_cm.append(cm_list)\n",
    "                \n",
    "    dataset_cm.append(params_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempted removal of node 11_5 failed\n",
      "attempted removal of node 17_13 failed\n",
      "attempted removal of node 3_3 failed\n",
      "attempted removal of node 15_23 failed\n",
      "attempted removal of node 2_34 failed\n",
      "attempted removal of node 8_7 failed\n",
      "attempted removal of node 15_17 failed\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "#####\n",
    "#####       FOR SINGlE PARAMETERS:\n",
    "#####\n",
    "\n",
    "# Prep a list to hold each dataset's results\n",
    "dataset_cm = []\n",
    "\n",
    "# Go through each Dataset (3T3, HEK293, HeLa, RAW264.7)\n",
    "for ben_trk_folder in bench_tracked_folders:\n",
    "\n",
    "    # Prep a list to hold results for each set of parameters\n",
    "    params_cm = []\n",
    "   \n",
    "    BENCHMARK_DIR = os.path.join(ben_trk_folder)\n",
    "\n",
    "    # Compile a list of each movie\n",
    "    sub_dirs = sorted_nicely(os.listdir(BENCHMARK_DIR))\n",
    "    movie_list = fnmatch.filter(sub_dirs, '???')\n",
    "    # Prep a list to hold each movie's results\n",
    "    cm_list = []\n",
    "    # Loop through each set of movies \n",
    "    for name in movie_list:\n",
    "        # Extract track.txt for each movie\n",
    "        pattern_gt = os.path.join(BENCHMARK_DIR, name + '_GT/TRA/')\n",
    "        pattern_res = os.path.join(BENCHMARK_DIR, name + '_RES/')\n",
    "\n",
    "        gtcells, rescells = match_nodes(pattern_gt+'*.tif',pattern_res+'*.tif')\n",
    "\n",
    "        if len(np.unique(rescells)) < len(np.unique(gtcells)):\n",
    "            node_key = {r:g for g,r in zip(gtcells,rescells)}\n",
    "            # node_key\n",
    "            # Maps gt nodes onto resnodes so must be applied to gt\n",
    "\n",
    "            G_res = create_graph(pattern_res+'res_track.txt',node_key=node_key)\n",
    "            G_gt = create_graph(pattern_gt+'man_track.txt')\n",
    "            cm_list.append(classify_divisions(G_gt,G_res))\n",
    "        else:\n",
    "            node_key = {g:r for g,r in zip(gtcells,rescells)}\n",
    "            G_res = create_graph(pattern_res+'res_track.txt')\n",
    "            G_gt = create_graph(pattern_gt+'man_track.txt',node_key=node_key)\n",
    "            cm_list.append(classify_divisions(G_gt,G_res))            \n",
    "\n",
    "    params_cm.append(cm_list)\n",
    "                \n",
    "    dataset_cm.append(params_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_totals = []\n",
    "for cell_type in dataset_cm:\n",
    "    cm_params = []\n",
    "    for param_comb in cell_type:\n",
    "\n",
    "        divI,divJ,divC,divGH = 0, 0, 0, 0\n",
    "        dataset_stats = {'Correct division': divI,\n",
    "                         'Incorrect division': divJ,\n",
    "                         'False positive division':divC,\n",
    "                         'False negative division':divGH}\n",
    "        for cm in param_comb:\n",
    "            dataset_stats['Correct division'] = dataset_stats['Correct division']+cm['Correct division']\n",
    "            dataset_stats['Incorrect division'] = dataset_stats['Incorrect division']+cm['Incorrect division']\n",
    "            dataset_stats['False positive division'] = dataset_stats['False positive division']+cm['False positive division']\n",
    "            dataset_stats['False negative division'] = dataset_stats['False negative division']+cm['False negative division']\n",
    "\n",
    "        cm_params.append(dataset_stats)\n",
    "    \n",
    "    cm_totals.append(cm_params)\n",
    "    #         print(cm['False negative division'])\n",
    "    #         print(cm['False positive division'])\n",
    "    #         print(cm['Incorrect division'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset  0\n",
      "MAX CD\n",
      "204  at  [0]\n",
      "MIN ID\n",
      "0  at  [0]\n",
      "min_FP\n",
      "9  at  [0]\n",
      "min_FN\n",
      "102  at  [0]\n"
     ]
    }
   ],
   "source": [
    "total_ideal_params = []\n",
    "for cell_idx, cell_type in enumerate(cm_totals):\n",
    "    min_FP = 1000\n",
    "    min_FP_idx = []\n",
    "    min_ID = 1000\n",
    "    min_ID_idx = []\n",
    "    min_FN = 1000\n",
    "    min_FN_idx = []\n",
    "    max_CD = 0\n",
    "    max_CD_idx = []\n",
    "    for param_idx, param_comb in enumerate(cell_type):\n",
    "        if param_comb['Correct division'] >= max_CD:\n",
    "            max_CD = param_comb['Correct division']\n",
    "            max_CD_idx.append(param_idx)\n",
    "        \n",
    "        if param_comb['Incorrect division'] <= min_ID:\n",
    "            min_ID = param_comb['Incorrect division']\n",
    "            min_ID_idx.append(param_idx)\n",
    "        \n",
    "        if param_comb['False positive division'] <= min_FP:\n",
    "            min_FP = param_comb['False positive division']\n",
    "            min_FP_idx.append(param_idx)\n",
    "        \n",
    "        if param_comb['False negative division'] <= min_FN:\n",
    "            min_FN = param_comb['False negative division']\n",
    "            min_FN_idx.append(param_idx)\n",
    "\n",
    "    print('Dataset ',cell_idx)\n",
    "    print('MAX CD')\n",
    "    print(max_CD, ' at ', max_CD_idx)\n",
    "    print('MIN ID')\n",
    "    print(min_ID, ' at ', min_ID_idx)\n",
    "    print('min_FP')\n",
    "    print(min_FP, ' at ', min_FP_idx)\n",
    "    print('min_FN')\n",
    "    print(min_FN, ' at ', min_FN_idx)\n",
    "    \n",
    "    ideal_params = []\n",
    "    for param_idx, param_comb in enumerate(cell_type):\n",
    "        if param_comb['Correct division'] == max_CD and param_comb['Incorrect division'] == min_ID and param_comb['False positive division'] == min_FP and param_comb['False negative division'] == min_FN:\n",
    "            ideal_params.append(param_idx)\n",
    "            \n",
    "    total_ideal_params.append(ideal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "div0.9_b0.99_d0.99\n",
      "3T3\n",
      "{'False negative division': 102, 'Incorrect division': 0, 'Correct division': 204, 'False positive division': 9}\n",
      "HEK293\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a1a360a30ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm_totals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HEK293'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm_totals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HeLa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm_totals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "# Go through each parameter combination\n",
    "for div_param in division:\n",
    "    # For each birth parameter\n",
    "    for birth_param in birth:\n",
    "        # For each death parameter\n",
    "        for death_param in death:  \n",
    "\n",
    "            if counter == 0:\n",
    "                PARAMS = 'div'+str(div_param)+'_b'+str(birth_param)+'_d'+str(death_param)\n",
    "                print(PARAMS)\n",
    "                print('3T3')\n",
    "                print(cm_totals[0][counter])\n",
    "                print('HEK293')\n",
    "                print(cm_totals[1][counter])\n",
    "                print('HeLa')\n",
    "                print(cm_totals[2][counter])\n",
    "                print('RAW')\n",
    "                print(cm_totals[3][counter])\n",
    "\n",
    "\n",
    "                \n",
    "            counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ISBI Benchmarking Scripts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  HeLa\n",
      "Mean:  0.904137\n",
      "Std. Dev.:  0.03951699999999997\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import statistics\n",
    "\n",
    "# Define path to folder containing the directories of interest (ie: 001, 001_GT, 001_RES)\n",
    "\n",
    "# ISBI_bens_3T3  = '/data/tracking_benchmarks/Final_Benchmarks/DC_TEST_DC_SEG/3T3/div0.9_b0.99_d0.99/'\n",
    "# ISBI_bens_HEK  = '/data/tracking_benchmarks/Final_Benchmarks/DC_TEST_DC_SEG/HEK293/div0.9_b0.99_d0.99/'\n",
    "# ISBI_bens_HeLa = '/data/tracking_benchmarks/Final_Benchmarks/DC_TEST_DC_SEG/HeLa/div0.9_b0.99_d0.99/'\n",
    "# ISBI_bens_RAW  = '/data/tracking_benchmarks/Final_Benchmarks/DC_TEST_DC_SEG/RAW264.7/div0.9_b0.99_d0.99/'\n",
    "\n",
    "# ISBI_bens_folders = [ISBI_bens_3T3, ISBI_bens_HEK, ISBI_bens_HeLa, ISBI_bens_RAW]\n",
    "# ISBI_bens_folder_names = ['3T3', 'HEK293', 'HeLa', 'RAW']\n",
    "\n",
    "\n",
    "# Alt path for ISBI dataset\n",
    "ISBI_bens_HeLa  = '/data/tracking_benchmarks/Final_Benchmarks/ISBI_DC_SEG/MaskRCNN/Fluo-N2DL-HeLa/div0.9_b0.99_d0.99/'\n",
    "\n",
    "ISBI_bens_folders = [ISBI_bens_HeLa]\n",
    "ISBI_bens_folder_names = ['HeLa']\n",
    "\n",
    "\n",
    "\n",
    "for index, path in enumerate(ISBI_bens_folders):\n",
    "\n",
    "    # Calculate the number of batches\n",
    "    dirs = os.listdir(path)\n",
    "    num_batches = int(len(dirs)/3)\n",
    "\n",
    "    TRA_Vals = []\n",
    "    for batch in range(num_batches):\n",
    "        batch = '{:03}'.format(batch+1)\n",
    "\n",
    "        # Run ISBI Tracking Benchmark\n",
    "        p = subprocess.run(['./TRAMeasure', path, batch], stdout=subprocess.PIPE)\n",
    "        # Save the output\n",
    "        outstring = p.stdout\n",
    "\n",
    "        try:\n",
    "            TRA_Val = float(outstring.decode('utf-8').split()[-1])\n",
    "            TRA_Vals.append(TRA_Val)\n",
    "        except:\n",
    "            print('Benchmarking failure - Batch ', batch)\n",
    "            print(outstring.decode('utf-8'))\n",
    "    \n",
    "    print('Dataset: ', ISBI_bens_folder_names[index])\n",
    "    print('Mean: ', statistics.mean(TRA_Vals))\n",
    "    print('Std. Dev.: ', statistics.pstdev(TRA_Vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
